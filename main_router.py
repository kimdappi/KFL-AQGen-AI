"""
ë¼ìš°í„° í†µí•© Agentic RAG ë©”ì¸ ì‹¤í–‰ íŒŒì¼
ê¸°ì¡´ main.pyì™€ ë™ì¼í•œ ì¶œë ¥ í˜•ì‹ ìœ ì§€
ìˆ˜ì • ì™„ë£Œ
"""

import json
import uuid
import glob
import os
from dotenv import load_dotenv
from langchain_core.runnables import RunnableConfig
# 
from Retriever.vocabulary_retriever import TOPIKVocabularyRetriever
from Retriever.grammar_retriever import GrammarRetriever
from Retriever.kpop_retriever import KpopSentenceRetriever

from Ragsystem.graph_agentic_router import RouterAgenticGraph
from config import TOPIK_PATHS, GRAMMAR_PATHS, KPOP_JSON_PATH , SENTENCE_SAVE_DIR
from test_maker import create_korean_test_set
load_dotenv()


def find_latest_sentence_file(directory=SENTENCE_SAVE_DIR):
    """ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ ê°€ì¥ ìµœê·¼ì— ìƒì„±ëœ JSON íŒŒì¼ ì°¾ê¸°"""
    try:
        list_of_files = glob.glob(os.path.join(directory, '*.json'))
        if not list_of_files:
            return None
        latest_file = max(list_of_files, key=os.path.getctime)
        return latest_file
    except Exception as e:
        print(f"íŒŒì¼ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë¼ìš°í„° í†µí•© ë²„ì „)"""
    
    print("\n" + "="*80)
    print("ğŸš€ ì™¸êµ­ì¸ì„ ìœ„í•œ í•œêµ­ì–´ í•™ìŠµ ë¬¸ì œ ìë™ ìƒì„± ì‹œìŠ¤í…œ")
    print("   KFL-AQGen-AI with Intelligent Router")
    print("="*80)
    
    # ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
    print("\nğŸ“š ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    print("   â”œâ”€ TOPIK ì–´íœ˜ ë°ì´í„°ë² ì´ìŠ¤")
    topik_retriever = TOPIKVocabularyRetriever(TOPIK_PATHS)
    print("   â”œâ”€ ë¬¸ë²• íŒ¨í„´ ë°ì´í„°ë² ì´ìŠ¤")
    grammar_retriever = GrammarRetriever(GRAMMAR_PATHS)
    print("   â””â”€ K-pop í•™ìŠµ ìë£Œ ë°ì´í„°ë² ì´ìŠ¤")
    kpop_retriever = KpopSentenceRetriever(KPOP_JSON_PATH)
    print("   âœ… ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ë¼ìš°í„° í†µí•© Agentic RAG ê·¸ë˜í”„ êµ¬ì¶•
    print("\nğŸ”§ ì§€ëŠ¥í˜• ë¼ìš°í„° ê¸°ë°˜ Agentic RAG ê·¸ë˜í”„ êµ¬ì¶• ì¤‘...")
    graph = RouterAgenticGraph(
        topik_retriever,
        grammar_retriever,
        kpop_retriever
    )
    print("   âœ… ê·¸ë˜í”„ êµ¬ì¶• ì™„ë£Œ")
    
    # ì„¤ì •
    config = RunnableConfig(
        recursion_limit=25,  # ì¬ê²€ìƒ‰ì„ ìœ„í•´ ì•½ê°„ ì¦ê°€
        configurable={"thread_id": str(uuid.uuid4())}
    )
    
    print("\n" + "="*80)
    print("ğŸ¯ Agentic RAG ì‹œìŠ¤í…œ ì‹œì‘ (ì§€ëŠ¥í˜• ë¼ìš°í„°)")
    print("="*80)
    
    all_generated_questions = []
    
    
    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    print("\nğŸ’¬ í•œêµ­ì–´ í•™ìŠµ ë¬¸ì œë¥¼ ìƒì„±í•  ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("   ì˜ˆì‹œ: 'Create intermediate level Korean grammar practice questions about BLACKPINK'")
    print("   ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n")
    
    while True:
        query = input("ğŸ“ ì¿¼ë¦¬ ì…ë ¥: ").strip()
        
        if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ê·¸ë§Œ']:
            print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if not query:
            print("   âš ï¸ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")
            continue
        
        print(f"\n{'='*80}")
        print(f"ğŸ”¹ ì²˜ë¦¬ ì¤‘...")
        print(f"   ì…ë ¥: {query}")
        print('='*80)

        # 1. ë¼ìš°í„° ê¸°ë°˜ Agentic RAG ì‹¤í–‰
        try:
            rag_output_string = graph.invoke(query, config)
            print("\n" + "="*80)
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            continue

        # 2. ìµœì‹  JSON íŒŒì¼ ì°¾ê¸°
        latest_payload_file = find_latest_sentence_file()

        if latest_payload_file:
            print(f"\nğŸ“„ ìƒì„±ëœ ì˜ˆë¬¸ íŒŒì¼: {latest_payload_file}")
            
            with open(latest_payload_file, 'r', encoding='utf-8') as f:
                sentence_payload = json.load(f)
            
            # Payload ê²€ì¦
            print("\n" + "="*70)
            print("ğŸ“‹ ìƒì„±ëœ í•™ìŠµ ìë£Œ ì •ë³´")
            print("="*70)
            print(f"   í•™ìŠµì ìˆ˜ì¤€ (ë“±ê¸‰): {sentence_payload.get('level')}")
            print(f"   ëª©í‘œ ë¬¸ë²•: {sentence_payload.get('target_grammar')}")
            print(f"   ìƒì„±ëœ ì˜ˆë¬¸: {len(sentence_payload.get('critique_summary', []))}ê°œ")
            
            # ìƒì„±ëœ ë¬¸ì¥ ì¶œë ¥
            for i, item in enumerate(sentence_payload.get('critique_summary', []), 1):
                print(f"      {i}. {item.get('sentence', 'N/A')}")
            
            # K-pop ì •ë³´ í™•ì¸
            if 'kpop_references' in sentence_payload:
                kpop_refs = sentence_payload['kpop_references']
                db_count = len(kpop_refs)  # ëª¨ë‘ DBì—ì„œ
                
                print(f"\n   âœ¨ K-pop ì°¸ì¡° ìë£Œ: ì´ {len(kpop_refs)}ê°œ")
                print(f"      - ë°ì´í„°ë² ì´ìŠ¤: {db_count}ê°œ")
                
                for i, ref in enumerate(kpop_refs[:5], 1):
                    print(f"      {i}. [DB] {ref.get('group', 'N/A')} - {ref.get('song', 'N/A')}")
            
            print("="*70)
            
            # 3. ë¬¸ì œ ìƒì„±
            print("\nğŸ¯ í•œêµ­ì–´ í•™ìŠµ ë¬¸ì œ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
            generated_questions = create_korean_test_set(sentence_payload, num_questions=6)

            if generated_questions:
                    print("\n" + "="*70)
                    print("âœ… ìƒì„±ëœ í•œêµ­ì–´ í•™ìŠµ ë¬¸ì œ ì„¸íŠ¸")
                    print("="*70)
                    print(json.dumps(generated_questions, indent=2, ensure_ascii=False))
                    print("="*70)

                    all_generated_questions.extend(generated_questions)
            else:
                print("\nâŒ ë¬¸ì œ ìƒì„± ì‹¤íŒ¨")
                
        else:
            print("\nâš ï¸ 'sentence' í´ë”ì—ì„œ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


        print("\n" + "="*80)
    
    # ìµœì¢… ì¶œë ¥ ì €ì¥
    output_filename = "output/final_v.1.json"
    print(f"\n{'='*80}")
    print(f"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘...")
    print(f"{'='*80}")
    print(f"   ìƒì„±ëœ ë¬¸ì œ ìˆ˜: {len(all_generated_questions)}ê°œ")
    print(f"   ì €ì¥ íŒŒì¼ëª…: {output_filename}")

    try:
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(all_generated_questions, f, ensure_ascii=False, indent=2)
        print(f"   âœ… '{output_filename}' ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"   âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    print("\n" + "="*80)
    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("   ì™¸êµ­ì¸ì„ ìœ„í•œ í•œêµ­ì–´ í•™ìŠµ ë¬¸ì œê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
