"""
Simplified Router-Integrated Nodes
ë‹¨ìˆœí™”ëœ ë¼ìš°íŒ…ê³¼ í•„ìˆ˜ ì–´íœ˜ í¬í•¨ ë³´ì¥
"""

from typing import Any, Dict
from Ragsystem.schema import GraphState
from Ragsystem.nodes import AgenticKoreanLearningNodes
from router import SimplifiedRouter
from agents import QueryAnalysisAgent, ProblemImprovementAgent


class SimplifiedRouterNodes(AgenticKoreanLearningNodes):
    """
    ë‹¨ìˆœí™”ëœ ë¼ìš°í„° í†µí•© ë…¸ë“œ
    - ë¬¸ë²• ë¬¸ì œì—¬ë„ ì–´íœ˜ í•„ìˆ˜ í¬í•¨
    - ì¿¼ë¦¬ ê¸°ë°˜ íŒë‹¨
    """
    
    def __init__(self, vocabulary_retriever, grammar_retriever, kpop_retriever, llm=None):
        super().__init__(vocabulary_retriever, grammar_retriever, kpop_retriever, llm)
        
        # ë‹¨ìˆœí™”ëœ ì»´í¬ë„ŒíŠ¸
        self.query_agent = QueryAnalysisAgent(llm=llm)
        self.quality_agent = ProblemImprovementAgent(llm=llm) 
        self.router = SimplifiedRouter(llm=llm)
        
        print("âœ… Simplified Router initialized - Always includes vocabulary")
    
    def routing_node(self, state: GraphState) -> GraphState:
        """
        ë‹¨ìˆœí™”ëœ ë¼ìš°íŒ… ë…¸ë“œ
        ì¿¼ë¦¬ë¥¼ ì´í•´í•˜ê³  í•„ìš”í•œ ë¦¬íŠ¸ë¦¬ë²„ ê²°ì •
        """
        print("\n" + "="*70)
        print("ğŸ”€ [Router] Simplified Routing Decision")
        print("="*70)
        
        query = state.get("input_text", "")
        query_analysis = state.get("query_analysis", {})
        
        # ë¼ìš°íŒ… ê²°ì • (ì¿¼ë¦¬ ì „ì²´ ë§¥ë½ ì´í•´)
        routing = self.router.route(query, query_analysis)
        
        print(f"ğŸ“Š ë¼ìš°íŒ… ê²°ì •:")
        print(f"   ì–´íœ˜: {'âœ“' if routing['use_vocabulary'] else 'âœ—'} ({routing.get('vocab_count', 5)}ê°œ)")
        print(f"   ë¬¸ë²•: {'âœ“' if routing['use_grammar'] else 'âœ—'} ({routing.get('grammar_count', 0)}ê°œ)")
        print(f"   K-pop: {'âœ“' if routing['use_kpop'] else 'âœ—'} ({routing.get('kpop_count', 0)}ê°œ)")
        print(f"   ê·¼ê±°: {routing.get('reasoning', '')}")
        
        # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ìƒì„±
        search_params = self.router.get_search_params(
            routing,
            query_analysis.get('difficulty', 'basic')
        )
        
        print("="*70)
        
        return {
            "routing_decision": routing,
            "search_params": search_params
        }
    
    def retrieve_vocabulary_routed(self, state: GraphState) -> GraphState:
        """
        ì–´íœ˜ ê²€ìƒ‰ - í•­ìƒ ì‹¤í–‰
        ë¬¸ë²• ë¬¸ì œì—¬ë„ ìµœì†Œ 3ê°œ ì´ìƒ í•„ìˆ˜
        """
        search_params = state.get("search_params", {})
        vocab_params = search_params.get("vocabulary", {})
        
        # ì–´íœ˜ëŠ” í•­ìƒ ê²€ìƒ‰ (ë¬¸ë²• ë¬¸ì œì—¬ë„ í•„ìˆ˜)
        print(f"\nğŸ“š [Vocabulary] TOPIK ì–´íœ˜ ê²€ìƒ‰ (í•„ìˆ˜)")
        
        query = state.get("input_text", "")
        level = vocab_params.get("level", "basic")
        limit = max(vocab_params.get("limit", 5), 3)  # ìµœì†Œ 3ê°œ ë³´ì¥
        
        print(f"   ë ˆë²¨: {level}")
        print(f"   ëª©í‘œ: {limit}ê°œ (ìµœì†Œ 3ê°œ ë³´ì¥)")
        
        vocab_docs = self.vocabulary_retriever.invoke(query, level)
        vocab_docs = vocab_docs[:limit]
        
        # ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ê²€ìƒ‰
        if len(vocab_docs) < 3:
            print(f"   âš ï¸ ì–´íœ˜ ë¶€ì¡± ({len(vocab_docs)}ê°œ), ì¶”ê°€ ê²€ìƒ‰...")
            # ë” ì¼ë°˜ì ì¸ ì¿¼ë¦¬ë¡œ ì¬ê²€ìƒ‰
            additional_docs = self.vocabulary_retriever.invoke(
                state.get("query_analysis", {}).get("topic", "daily"),
                level
            )
            vocab_docs.extend(additional_docs[:3-len(vocab_docs)])
        
        print(f"   âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(vocab_docs)}ê°œ ì–´íœ˜")
        
        return {"vocabulary_docs": vocab_docs}
    
    def retrieve_grammar_routed(self, state: GraphState) -> GraphState:
        """ë¬¸ë²• ê²€ìƒ‰ - í•„ìš”ì‹œì—ë§Œ"""
        search_params = state.get("search_params", {})
        grammar_params = search_params.get("grammar", {})
        
        if not grammar_params.get("enabled", False):
            print("   â­ï¸ ë¬¸ë²• ê²€ìƒ‰ ìŠ¤í‚µ")
            return {"grammar_docs": []}
        
        print(f"\nğŸ“– [Grammar] ë¬¸ë²• íŒ¨í„´ ê²€ìƒ‰")
        
        query = state.get("input_text", "")
        level = grammar_params.get("level", "basic")
        limit = grammar_params.get("limit", 2)
        
        print(f"   ë ˆë²¨: {level}")
        print(f"   ëª©í‘œ: {limit}ê°œ")
        
        grammar_docs = self.grammar_retriever.invoke(query, level)
        grammar_docs = grammar_docs[:limit]
        
        print(f"   âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(grammar_docs)}ê°œ ë¬¸ë²•")
        
        return {"grammar_docs": grammar_docs}
    
    def retrieve_kpop_routed(self, state: GraphState) -> GraphState:
        """K-pop ê²€ìƒ‰ - K-pop ì–¸ê¸‰ì‹œì—ë§Œ"""
        search_params = state.get("search_params", {})
        kpop_params = search_params.get("kpop", {})
        
        if not kpop_params.get("enabled", False):
            print("   â­ï¸ K-pop ê²€ìƒ‰ ìŠ¤í‚µ (ì–¸ê¸‰ ì—†ìŒ)")
            return {"kpop_docs": []}
        
        print(f"\nğŸµ [K-pop] K-pop ë¬¸ì¥ ê²€ìƒ‰")
        
        query = state.get("input_text", "")
        level = kpop_params.get("level", "basic")
        limit = kpop_params.get("limit", 3)
        
        # ì‚¬ìš©ì ê´€ì‹¬ì‚¬ ì¶”ì¶œ
        query_analysis = state.get("query_analysis", {})
        user_interests = query_analysis.get("user_interests", [])
        
        print(f"   ë ˆë²¨: {level}")
        print(f"   ëª©í‘œ: {limit}ê°œ")
        if user_interests:
            print(f"   ê´€ì‹¬ì‚¬: {', '.join(user_interests)}")
        
        # K-pop ê²€ìƒ‰ (ê´€ì‹¬ì‚¬ í•„í„°ë§ ì ìš©)
        kpop_docs = self.kpop_retriever.invoke(query, level)
        
        # ê´€ì‹¬ì‚¬ í•„í„°ë§ (ê°„ë‹¨í•œ ë²„ì „)
        if user_interests and hasattr(self, 'filter_kpop_by_interests'):
            kpop_docs = self.filter_kpop_by_interests(kpop_docs, user_interests)
        
        kpop_docs = kpop_docs[:limit]
        
        print(f"   âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(kpop_docs)}ê°œ K-pop ë¬¸ì¥")
        
        return {"kpop_docs": kpop_docs}
    
    def filter_kpop_by_interests(self, docs, interests):
        """ê°„ë‹¨í•œ ê´€ì‹¬ì‚¬ í•„í„°ë§"""
        if not interests:
            return docs
        
        filtered = []
        others = []
        
        for doc in docs:
            metadata = doc.metadata
            content = (metadata.get('group', '') + ' ' + 
                      metadata.get('song', '') + ' ' + 
                      metadata.get('sentence', '')).lower()
            
            # ê´€ì‹¬ì‚¬ì™€ ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸
            if any(interest.lower() in content for interest in interests):
                filtered.append(doc)
            else:
                others.append(doc)
        
        # ê´€ì‹¬ì‚¬ ë§¤ì¹­ ìš°ì„ , ë¶€ì¡±í•˜ë©´ ê¸°íƒ€ ì¶”ê°€
        return filtered + others
    
    def check_quality_agent(self, state: GraphState) -> GraphState:
        """
        í’ˆì§ˆ ì²´í¬ - ì–´íœ˜ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
        ë¬¸ë²• ë¬¸ì œì—¬ë„ ì–´íœ˜ 3ê°œ ì´ìƒ í•„ìˆ˜
        """
        print("\nâœ… [Quality Check] ê²€ìƒ‰ ê²°ê³¼ ê²€ì¦")
        
        vocab_count = len(state.get('vocabulary_docs', []))
        grammar_count = len(state.get('grammar_docs', []))
        kpop_count = len(state.get('kpop_docs', []))
        
        query_analysis = state.get('query_analysis', {})
        needs_kpop = query_analysis.get('needs_kpop', False)
        
        # ìµœì†Œ ìš”êµ¬ì‚¬í•­
        min_vocab = 3  # ë¬¸ë²• ë¬¸ì œì—¬ë„ ìµœì†Œ 3ê°œ
        min_grammar = 1 if state.get('routing_decision', {}).get('use_grammar') else 0
        min_kpop = 3 if needs_kpop else 0
        
        # ì¶©ë¶„ì„± ê²€ì‚¬
        vocab_sufficient = vocab_count >= min_vocab
        grammar_sufficient = grammar_count >= min_grammar or not state.get('routing_decision', {}).get('use_grammar')
        kpop_sufficient = kpop_count >= min_kpop or not needs_kpop
        
        sufficient = vocab_sufficient and grammar_sufficient and kpop_sufficient
        
        print(f"   ì–´íœ˜: {vocab_count}ê°œ (ìµœì†Œ {min_vocab}ê°œ) {'âœ“' if vocab_sufficient else 'âœ—'}")
        print(f"   ë¬¸ë²•: {grammar_count}ê°œ (ìµœì†Œ {min_grammar}ê°œ) {'âœ“' if grammar_sufficient else 'âœ—'}")
        if needs_kpop:
            print(f"   K-pop: {kpop_count}ê°œ (ìµœì†Œ {min_kpop}ê°œ) {'âœ“' if kpop_sufficient else 'âœ—'}")
        
        print(f"   ì¢…í•©: {'ì¶©ë¶„í•¨' if sufficient else 'ë¶€ì¡±í•¨'}")
        
        return {
            "quality_check": {
                "sufficient": sufficient,
                "vocab_count": vocab_count,
                "grammar_count": grammar_count,
                "kpop_db_count": kpop_count,
                "details": {
                    "vocab_sufficient": vocab_sufficient,
                    "grammar_sufficient": grammar_sufficient,
                    "kpop_sufficient": kpop_sufficient,
                    "min_requirements": {
                        "vocab": min_vocab,
                        "grammar": min_grammar,
                        "kpop": min_kpop
                    }
                }
            }
        }
    
    def generate_sentences_with_kpop(self, state: GraphState) -> GraphState:
        """
        ë¬¸ì¥ ìƒì„± - ì–´íœ˜ë¥¼ ë°˜ë“œì‹œ í¬í•¨
        """
        print("\nâœï¸ [Generation] ì˜ˆë¬¸ ìƒì„±")
        
        vocab_docs = state.get('vocabulary_docs', [])
        grammar_docs = state.get('grammar_docs', [])
        
        # ì–´íœ˜ í™•ì¸
        vocab_words = [doc.metadata.get('word', '') for doc in vocab_docs[:5]]
        
        if vocab_words:
            print(f"   í¬í•¨ë  ì–´íœ˜: {', '.join(vocab_words[:3])}...")
        
        # ë¬¸ë²• í™•ì¸
        if grammar_docs:
            target_grammar = grammar_docs[0].metadata.get('grammar', '')
            print(f"   ëª©í‘œ ë¬¸ë²•: {target_grammar}")
        
        # ê¸°ì¡´ ìƒì„± ë¡œì§ í˜¸ì¶œ
        result = super().generate_sentences_with_kpop(state)
        
        # ì–´íœ˜ í¬í•¨ ê²€ì¦
        if result.get('generated_sentences'):
            print(f"   âœ… {len(result['generated_sentences'])}ê°œ ë¬¸ì¥ ìƒì„± (ì–´íœ˜ í¬í•¨)")
        
        return result
    
    def rerank_simple(self, state: GraphState) -> GraphState:
        """
        ë‹¨ìˆœ ì¬ê²€ìƒ‰ - ì–´íœ˜ê°€ ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ê²€ìƒ‰
        """
        print("\nğŸ”„ [Rerank] ì¬ê²€ìƒ‰ ìˆ˜í–‰")
        
        quality_check = state.get("quality_check", {})
        current_count = state.get("rerank_count", 0)
        
        # ì–´íœ˜ ë¶€ì¡±ì‹œ ì¶”ê°€ ê²€ìƒ‰
        vocab_count = quality_check.get("vocab_count", 0)
        if vocab_count < 3:
            print(f"   ì–´íœ˜ ì¬ê²€ìƒ‰: {vocab_count} â†’ 3ê°œ ëª©í‘œ")
            
            # ë” ë„“ì€ ë²”ìœ„ë¡œ ì¬ê²€ìƒ‰
            level = state.get("difficulty_level", "basic")
            topic = state.get("query_analysis", {}).get("topic", "daily")
            
            additional_docs = self.vocabulary_retriever.invoke(topic, level)
            state["vocabulary_docs"].extend(additional_docs[:5-vocab_count])
            
        # ë¬¸ë²• ë¶€ì¡±ì‹œ ì¶”ê°€ ê²€ìƒ‰
        if state.get("routing_decision", {}).get("use_grammar"):
            grammar_count = quality_check.get("grammar_count", 0)
            if grammar_count < 1:
                print(f"   ë¬¸ë²• ì¬ê²€ìƒ‰: {grammar_count} â†’ 1ê°œ ëª©í‘œ")
                
                level = state.get("difficulty_level", "basic")
                grammar_docs = self.grammar_retriever.invoke("grammar patterns", level)
                state["grammar_docs"] = grammar_docs[:1]
        
        return {"rerank_count": current_count + 1}