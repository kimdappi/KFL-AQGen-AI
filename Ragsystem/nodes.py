# =====================================
# nodes.py (ê°œì„  ë²„ì „) - Evaluator ê¸°ë°˜ ìµœì í™”
# =====================================
"""
LangGraph ë…¸ë“œ ì •ì˜ (ê°œì„ ëœ ì¬ìƒì„± ë¡œì§)
- 3íšŒ ì‹œë„ í›„ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ ìë™ ì„ íƒ
- ì ì§„ì  í”„ë¡¬í”„íŠ¸ ê°•í™”
- ëª…í™•í•œ ì–´íœ˜ í• ë‹¹
"""
import json
import os
import re
import random
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime
from langchain.chat_models import ChatOpenAI
from Ragsystem.schema import GraphState
from utils import (
    detect_difficulty_from_text,
    extract_words_from_docs,
    extract_grammar_with_grade  
)
from config import LLM_CONFIG, SENTENCE_SAVE_DIR
from agents import QueryAnalysisAgent, QualityCheckAgent

# Evaluator ì„í¬íŠ¸
try:
    from Evaluator.kpop_evaluator import KpopSentenceEvaluator
    EVALUATOR_ENABLED = True
except ImportError:
    EVALUATOR_ENABLED = False
    print("âš ï¸ Evaluator ëª¨ë“ˆ ì—†ìŒ - ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰")

INVALID_CHARS = r'[<>:"/\\|?*\x00-\x1F]'

def sanitize_filename(name: str, replacement: str = "_") -> str:
    """Windows íŒŒì¼ëª… ì•ˆì „ ì²˜ë¦¬"""
    safe = re.sub(INVALID_CHARS, replacement, name)
    safe = safe.strip().strip(".")
    RESERVED = {"CON","PRN","AUX","NUL",*(f"COM{i}" for i in range(1,10)),*(f"LPT{i}" for i in range(1,10))}
    if safe.upper() in RESERVED:
        safe = f"_{safe}"
    return safe[:120] if len(safe) > 120 else safe

class KoreanLearningNodes:
    """í•œêµ­ì–´ í•™ìŠµ ë…¸ë“œ í´ë˜ìŠ¤"""
    
    def __init__(self, vocabulary_retriever, grammar_retriever, kpop_retriever, llm=None):
        self.vocabulary_retriever = vocabulary_retriever
        self.grammar_retriever = grammar_retriever
        self.kpop_retriever = kpop_retriever
        self.llm = llm or ChatOpenAI(
            model="gpt-4o-mini",
            temperature=LLM_CONFIG.get('temperature', 0.7),
            max_tokens=LLM_CONFIG.get('max_tokens', 1000)
        )
        
        # Evaluator ì´ˆê¸°í™”
        self.evaluator = None
        if EVALUATOR_ENABLED:
            try:
                self.evaluator = KpopSentenceEvaluator()
                print("   âœ… ë¬¸ì¥ í‰ê°€ê¸° í™œì„±í™”")
            except Exception as e:
                print(f"   â„¹ï¸ í‰ê°€ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.evaluator = None
        
        self.output_dir = "sentence"
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def detect_difficulty(self, state: GraphState) -> GraphState:
        """ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ ë‚œì´ë„ ê°ì§€"""
        difficulty = detect_difficulty_from_text(state['input_text'])
        return {"difficulty_level": difficulty}
    
    def retrieve_vocabulary(self, state: GraphState) -> GraphState:
        """ë‹¨ì–´ ê²€ìƒ‰ ë…¸ë“œ"""
        level = state['difficulty_level']
        query = state['input_text']
        vocab_docs = self.vocabulary_retriever.invoke(query, level)
        return {"vocabulary_docs": vocab_docs}

    def retrieve_kpop(self, state: GraphState) -> GraphState:
        """K-pop ë¬¸ì¥ ê²€ìƒ‰ ë…¸ë“œ"""
        level = state['difficulty_level']
        query = state['input_text']
        kpop_docs = self.kpop_retriever.invoke(query, level)
        return {"kpop_docs": kpop_docs}
    
    def retrieve_grammar(self, state: GraphState) -> GraphState:
        """ë¬¸ë²• ê²€ìƒ‰ ë…¸ë“œ"""
        level = state['difficulty_level']
        query = state['input_text']
        grammar_docs = self.grammar_retriever.invoke(query, level)
        return {"grammar_docs": grammar_docs}

    def generate_sentences(self, state: GraphState) -> GraphState:
        """ê²€ìƒ‰ëœ ë‹¨ì–´ì™€ ë¬¸ë²•ì„ í™œìš©í•œ ë¬¸ì¥ ìƒì„± (K-pop ì •ë³´ í¬í•¨)"""
        words_info = extract_words_from_docs(state['vocabulary_docs'])
        
        # K-pop ì •ë³´ ì¶”ì¶œ
        kpop_references = []
        kpop_context_text = ""
        
        if 'kpop_docs' in state and state['kpop_docs']:
            print(f"[ì°¸ì¡°] K-pop ë¬¸ì„œ ê°œìˆ˜: {len(state['kpop_docs'])}")
            
            for doc in state['kpop_docs'][:3]:
                sentence = doc.metadata.get('sentence', '')
                song = doc.metadata.get('song', '')
                group = doc.metadata.get('group', '')
                
                if sentence:
                    kpop_references.append({
                        "sentence": sentence,
                        "song": song,
                        "group": group,
                    })
                    kpop_context_text += f'- "{sentence}" ({song} - {group})\n'
        
        print(f"[ì°¸ì¡°] K-pop ì°¸ì¡° ê°œìˆ˜: {len(kpop_references)}")
        
        grammar_info = extract_grammar_with_grade(state['grammar_docs'])
        
        # ì–´íœ˜ í¬ë§·íŒ…
        words_formatted = []
        vocab_list = []
        for word, wordclass in words_info[:5]:
            words_formatted.append(f"{word}({wordclass})")
            vocab_list.append(word)
        
        target_grammar = grammar_info[0]['grammar'] if grammar_info else "ê¸°ë³¸ ë¬¸ë²•"
        target_grade = grammar_info[0]['grade'] if grammar_info else 1
        
        print("grammar : ", target_grammar)
        print("grade : ", target_grade)
        
        difficulty = state['difficulty_level']
        difficulty_guide = {
            "basic": "ì´ˆê¸‰ í•™ìŠµì (TOPIK 1-2ê¸‰): ì§§ê³  ê°„ë‹¨í•œ ë¬¸ì¥, ê¸°ë³¸ ì‹œì œ ì‚¬ìš©",
            "intermediate": "ì¤‘ê¸‰ í•™ìŠµì (TOPIK 3-4ê¸‰): ë‹¤ì–‘í•œ ì—°ê²°ì–´ë¯¸, ìì—°ìŠ¤ëŸ¬ìš´ ì¼ìƒ ëŒ€í™” í‘œí˜„",
            "advanced": "ê³ ê¸‰ í•™ìŠµì (TOPIK 5-6ê¸‰): ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°, ê²©ì‹ì²´ë‚˜ ë¬¸ì–´ì²´ ê°€ëŠ¥"
        }
        
        prompt = self._build_generation_prompt(
            difficulty, 
            target_grade, 
            words_formatted, 
            target_grammar, 
            kpop_context_text,
            difficulty_guide
        )
        
        # ë¬¸ì¥ ìƒì„± (3ê°œ)
        response = self.llm.predict(prompt)
        sentences = response.strip().split('\n')
        sentences = [s.strip() for s in sentences if s.strip()][:3]
        
        # í‰ê°€ ìˆ˜í–‰
        critique_summary = self._evaluate_sentences(
            sentences, 
            target_grammar, 
            vocab_list
        )
        
        # JSON ì €ì¥ ë°ì´í„°
        save_data = {
            "level": target_grade,
            "target_grammar": target_grammar,
            "kpop_references": kpop_references,
            "critique_summary": critique_summary
        }
        
        messages = [
            ("user", state['input_text']),
            ("assistant", "\n".join(sentences))
        ]
        
        return {
            "generated_sentences": sentences,
            "messages": messages,
            "sentence_data": save_data,
            "target_grade": target_grade
        }
    
    def _build_generation_prompt(self, difficulty, target_grade, words_formatted, 
                                target_grammar, kpop_context_text, difficulty_guide):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
        prompt_templates = {
            "basic": """
[ROLE]
ë„ˆëŠ” í•œêµ­ì–´ë¥¼ ë°°ìš°ëŠ” ì´ˆê¸‰ í•™ìŠµìë¥¼ ìœ„í•œ ì¹œì ˆí•œ í•œêµ­ì–´ ì„ ìƒë‹˜ì´ì•¼.

[INSTRUCTIONS]
- í•™ìŠµ ìˆ˜ì¤€: {difficulty_level} (Grade {target_grade})
- ì˜¤ëŠ˜ì˜ ë‹¨ì–´: {words_formatted}
- ì˜¤ëŠ˜ì˜ ë¬¸ë²•: {target_grammar}
- K-pop ì°¸ê³ : {kpop_context_text}

[SENTENCE RULES]
1. ì§§ê³  ê°„ë‹¨í•œ ë¬¸ì¥ (10-15 ë‹¨ì–´)
2. ë¬¸ë²• íŒ¨í„´ {target_grammar} í•„ìˆ˜ í¬í•¨
3. ì œì‹œëœ ë‹¨ì–´ ë¬¸ì¥ í•˜ë‚˜ë‹¹ ìµœì†Œ 1ê°œì”© ê²¹ì¹˜ì§€ ì•Šê²Œ í•„ìˆ˜ í¬í•¨

í˜•ì‹: ë²ˆí˜¸ ì—†ì´ ë¬¸ì¥ 3ê°œë§Œ
""",
            "intermediate": """
[ROLE]
ë„ˆëŠ” ì¤‘ê¸‰ í•œêµ­ì–´ í•™ìŠµìë¥¼ ìœ„í•œ ê²½í—˜ ë§ì€ í•œêµ­ì–´ êµì‚¬ì•¼.

[INSTRUCTIONS]
- í•™ìŠµ ìˆ˜ì¤€: {difficulty_level} (Grade {target_grade})
- í•µì‹¬ ì–´íœ˜: {words_formatted}
- ëª©í‘œ ë¬¸ë²•: {target_grammar}
- K-pop ì°¸ê³ : {kpop_context_text}

[REQUIREMENTS]
1. ì¤‘ê¸‰ ìˆ˜ì¤€ì˜ ë¬¸ì¥ ìƒì„±
2. ë¬¸ë²• {target_grammar} í•„ìˆ˜ í¬í•¨
3. ì œì‹œëœ ì–´íœ˜ ë¬¸ì¥ë‹¹ ìµœì†Œ 1ê°œì”© ê²¹ì¹˜ì§€ ì•Šê²Œ í•„ìˆ˜ í¬í•¨

ì¶œë ¥: ì˜ˆë¬¸ 3ê°œë§Œ (ë²ˆí˜¸ ì—†ì´)
""",
            "advanced": """
[ROLE]
ë„ˆëŠ” ê³ ê¸‰ í•œêµ­ì–´ í•™ìŠµìë¥¼ ìœ„í•œ ì „ë¬¸ êµìˆ˜ë‹¤.

[INSTRUCTIONS]
- í•™ìŠµ ìˆ˜ì¤€: {difficulty_level} (Grade {target_grade})
- í•µì‹¬ ì–´íœ˜: {words_formatted}
- í•µì‹¬ ë¬¸ë²•: {target_grammar}
- K-pop ì°¸ê³ : {kpop_context_text}

[REQUIREMENTS]
1. ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°
2. ë¬¸ë²• {target_grammar} í•„ìˆ˜ í¬í•¨í•´ì„œ ì‹¬í™” í™œìš©
3. ì œì‹œëœ ì–´íœ˜ ì¤‘ ë¬¸ì¥ë‹¹ ìµœì†Œ 1ê°œ ê²¹ì¹˜ì§€ ì•Šê²Œ í•„ìˆ˜ í¬í•¨

ì¶œë ¥: ì˜ˆë¬¸ 3ê°œë§Œ
"""
        }
        
        template = prompt_templates.get(difficulty, prompt_templates["intermediate"])
        return template.format(
            difficulty_level=difficulty_guide.get(difficulty, difficulty),
            target_grade=target_grade,
            words_formatted=', '.join(words_formatted),
            target_grammar=target_grammar,
            kpop_context_text=kpop_context_text if kpop_context_text else "ì—†ìŒ"
        )
    
    def _evaluate_sentences(self, sentences, target_grammar, vocab_list):
        """ìƒì„±ëœ ë¬¸ì¥ í‰ê°€"""
        if self.evaluator and sentences:
            try:
                print("\n   ğŸ“Š ìƒì„±ëœ ë¬¸ì¥ í’ˆì§ˆ í‰ê°€ ì¤‘...")
                evaluation_results = self.evaluator.evaluate_batch(
                    sentences,
                    grammar=target_grammar,
                    vocab=vocab_list
                )
                
                critique_summary = []
                for sent, eval_res in zip(sentences, evaluation_results):
                    critique_summary.append({
                        "sentence": sent,
                        "grammar_ok": eval_res.get("grammar_ok", False),
                        "vocab_ok": eval_res.get("vocab_ok", False)
                    })
                
                return critique_summary
                
            except Exception as e:
                print(f"   âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return [{"sentence": s} for s in sentences]
    
    def format_output(self, state: GraphState) -> GraphState:
        """ìµœì¢… ì¶œë ¥ í¬ë§·íŒ… ë° JSON ì €ì¥"""
        output = f"=== í•œêµ­ì–´ í•™ìŠµ ë¬¸ì œ ìƒì„± ê²°ê³¼ ===\n"
        output += f"ë‚œì´ë„: {state['difficulty_level']}\n"
        
        if 'target_grade' in state:
            output += f"ë¬¸ë²• Grade: {state['target_grade']}\n"
        
        output += "\nìƒì„±ëœ ì˜ˆë¬¸:\n"
        for i, sentence in enumerate(state['generated_sentences'], 1):
            output += f"{i}. {sentence}\n"
        
        if 'sentence_data' in state and state['sentence_data']:
            saved_file = self._save_to_json(state['sentence_data'])
            output += f"\nì˜ˆë¬¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {saved_file}\n"
        
        return {"final_output": output}
    
    def _save_to_json(self, sentence_data: dict) -> str:
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        out_dir = Path(SENTENCE_SAVE_DIR)
        out_dir.mkdir(parents=True, exist_ok=True)

        level = sentence_data.get("level", "grade1")
        title = sentence_data.get("title", "untitled")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        base = f"sentences_{level}_{title}_{timestamp}"
        safe_base = sanitize_filename(base)
        filepath = out_dir / f"{safe_base}.json"

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(sentence_data, f, ensure_ascii=False, indent=2)

        return str(filepath)


# =====================================
# Agentic RAG êµ¬í˜„ (ê°œì„  ë²„ì „)
# =====================================
class AgenticKoreanLearningNodes(KoreanLearningNodes):
    """Agentic RAG ë…¸ë“œ - ì¬ìƒì„± ë¡œì§ ìµœì í™”"""
    
    def __init__(self, vocabulary_retriever, grammar_retriever, kpop_retriever, llm=None):
        super().__init__(vocabulary_retriever, grammar_retriever, kpop_retriever, llm)
        self.query_agent = QueryAnalysisAgent(llm)
        self.quality_agent = QualityCheckAgent(llm)
    
    def analyze_query_agent(self, state: GraphState) -> GraphState:
        """ì¿¼ë¦¬ ë¶„ì„ ì—ì´ì „íŠ¸ ë…¸ë“œ"""
        print("\nğŸ” [Agent] Query Analysis")
        analysis = self.query_agent.analyze(state['input_text'])
        
        print(f"   Difficulty: {analysis['difficulty']}")
        print(f"   Topic: {analysis['topic']}")
        print(f"   Needs K-pop: {analysis.get('needs_kpop', False)}")
        print(f"   K-pop Groups: {analysis.get('kpop_groups', [])}")
        
        return {
            "difficulty_level": analysis['difficulty'],
            "query_analysis": analysis
        }
    
    def retrieve_kpop_mixed(self, state: GraphState) -> GraphState:
        """K-pop ê²€ìƒ‰ ë…¸ë“œ"""
        print("\nğŸµ [Agent] K-pop Retrieval")
        
        level = state['difficulty_level']
        query = state['input_text']
        
        kpop_db_docs = self.kpop_retriever.invoke(query, level)
        kpop_db_docs = kpop_db_docs[:5]
        print(f"   DB ê²€ìƒ‰: {len(kpop_db_docs)}ê°œ")
        
        return {"kpop_docs": kpop_db_docs}
    
    def check_quality_agent(self, state: GraphState) -> GraphState:
        """í’ˆì§ˆ ê²€ì¦ ì—ì´ì „íŠ¸ ë…¸ë“œ"""
        print("\nâœ… [Agent] í’ˆì§ˆ ê²€ì¦")
        
        query_analysis = state.get('query_analysis', {})
        needs_kpop = query_analysis.get('needs_kpop', False)
        
        result = self.quality_agent.check(
            vocab_count=len(state.get('vocabulary_docs', [])),
            grammar_count=len(state.get('grammar_docs', [])),
            kpop_db_count=len(state.get('kpop_docs', [])),
            needs_kpop=needs_kpop
        )
        
        print(f"   ì–´íœ˜: {result['vocab_count']}ê°œ")
        print(f"   ë¬¸ë²•: {result['grammar_count']}ê°œ")
        print(f"   K-pop: {result['kpop_db_count']}ê°œ")
        print(f"   ìƒíƒœ: {result['message']}")
        
        return {"quality_check": result}
    
    def generate_sentences_with_kpop(self, state):
        """
        ê°œì„ ëœ ë¬¸ì¥ ìƒì„± ë¡œì§
        - 3íšŒ ì‹œë„ í›„ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ ì„ íƒ
        - ì ì§„ì  í”„ë¡¬í”„íŠ¸ ê°•í™”
        """
        print("\nâœï¸ [Agent] í•œêµ­ì–´ í•™ìŠµ ë¬¸ì¥ ìƒì„± (ìµœì í™”)")
        
        from utils import extract_words_from_docs, extract_grammar_with_grade
        
        words_info = extract_words_from_docs(state['vocabulary_docs'])
        grammar_info = extract_grammar_with_grade(state['grammar_docs'])
        
        # ì¿¼ë¦¬ ë¶„ì„ ì •ë³´
        query_analysis = state.get('query_analysis', {})
        needs_kpop = query_analysis.get('needs_kpop', False)
        specified_groups = query_analysis.get('kpop_groups', [])
        
        print(f"   ì¿¼ë¦¬ ë¶„ì„: needs_kpop={needs_kpop}, ì§€ì • ê·¸ë£¹={specified_groups}")
        
        # K-pop ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
        kpop_metadata, kpop_context_text, kpop_groups = self._process_kpop_docs(
            state.get('kpop_docs', []),
            specified_groups
        )
        
        print(f"   K-pop ì •ë³´: {len(kpop_metadata)}ê°œ - {kpop_groups}" if kpop_metadata else "   K-pop ì •ë³´: ì—†ìŒ")
        
        # ì–´íœ˜/ë¬¸ë²• ì¤€ë¹„
        vocab_list = [word for word, _ in words_info[:5]]
        target_grammar = grammar_info[0]['grammar'] if grammar_info else "ê¸°ë³¸ ë¬¸ë²•"
        target_grade = grammar_info[0]['grade'] if grammar_info else 1
        difficulty = state['difficulty_level']
        
        print(f"\n   ğŸ¯ íƒ€ê²Ÿ: ë¬¸ë²• '{target_grammar}' + ì–´íœ˜ {vocab_list}")
        
        # ===================================
        # 3íšŒ ì‹œë„, ê°€ì¥ ì¢‹ì€ ê²°ê³¼ ì„ íƒ
        # ===================================
        max_attempts = 3
        all_attempts = []
        
        for attempt in range(max_attempts):
            print(f"\n   ğŸ“ ì‹œë„ {attempt + 1}/{max_attempts}")
            
            # ì ì§„ì ìœ¼ë¡œ ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._build_progressive_prompt(
                attempt,
                difficulty,
                target_grade,
                target_grammar,
                vocab_list,
                kpop_groups,
                kpop_context_text,
                needs_kpop,
                all_attempts  # ì´ì „ ì‹¤íŒ¨ ì •ë³´
            )
            
            # ë¬¸ì¥ ìƒì„±
            response = self.llm.predict(prompt)
            sentences = [s.strip() for s in response.strip().split('\n') if s.strip()][:3]
            
            # í‰ê°€ ìˆ˜í–‰
            critique = self._evaluate_sentences(sentences, target_grammar, vocab_list)
            
            # K-pop í¬í•¨ ì²´í¬
            kpop_ok = self._check_kpop_inclusion(sentences, kpop_groups) if needs_kpop else True
            
            # ì ìˆ˜ ê³„ì‚°
            score = self._calculate_score(critique, kpop_ok)
            
            all_attempts.append({
                'sentences': sentences,
                'critique': critique,
                'score': score,
                'kpop_ok': kpop_ok
            })
            
            print(f"      ì ìˆ˜: {score}/3 (ë¬¸ë²•+ì–´íœ˜+K-pop)")
            
            # ì™„ë²½í•œ ê²°ê³¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ
            if score == 3:
                print(f"   âœ… ì™„ë²½í•œ ë¬¸ì¥ ìƒì„±!")
                break
        
        # ê°€ì¥ ì¢‹ì€ ê²°ê³¼ ì„ íƒ
        best_attempt = max(all_attempts, key=lambda x: x['score'])
        final_sentences = best_attempt['sentences']
        critique_summary = best_attempt['critique']
        
        print(f"\n   ğŸ† ìµœì¢… ì„ íƒ: ì ìˆ˜ {best_attempt['score']}/3")
        
        # JSON ì €ì¥ ë°ì´í„°
        save_data = {
            "level": target_grade,
            "target_grammar": target_grammar,
            "kpop_references": kpop_metadata,
            "specified_groups": specified_groups,
            "critique_summary": critique_summary
        }
        
        messages = [
            ("user", state['input_text']),
            ("assistant", "\n".join(final_sentences))
        ]
        
        return {
            "generated_sentences": final_sentences,
            "messages": messages,
            "sentence_data": save_data,
            "target_grade": target_grade
        }
    
    def _calculate_score(self, critique, kpop_ok):
        """
        ë¬¸ì¥ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        - ë¬¸ë²• ì¶©ì¡±: +1ì 
        - ì–´íœ˜ ì¶©ì¡±: +1ì 
        - K-pop í¬í•¨ (í•„ìš”ì‹œ): +1ì 
        """
        grammar_pass = sum(1 for c in critique if c.get('grammar_ok', False))
        vocab_pass = sum(1 for c in critique if c.get('vocab_ok', False))
        
        score = 0
        if grammar_pass == 3:  # 3ê°œ ë¬¸ì¥ ëª¨ë‘ ë¬¸ë²• ì¶©ì¡±
            score += 1
        if vocab_pass == 3:    # 3ê°œ ë¬¸ì¥ ëª¨ë‘ ì–´íœ˜ ì¶©ì¡±
            score += 1
        if kpop_ok:             # K-pop ì¡°ê±´ ì¶©ì¡±
            score += 1
        
        return score
    
    def _build_progressive_prompt(self, attempt, difficulty, target_grade, 
                                  target_grammar, vocab_list, kpop_groups,
                                  kpop_context_text, needs_kpop, previous_attempts):
        """
        ì ì§„ì ìœ¼ë¡œ ê°•í™”ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
        - attempt 0: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
        - attempt 1: ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ + ì´ì „ ì‹¤íŒ¨ ì •ë³´
        - attempt 2: ìµœëŒ€ ê°•í™” + êµ¬ì²´ì ì¸ ì–´íœ˜ í• ë‹¹
        """
        difficulty_guide = {
            "basic": "ì´ˆê¸‰ (TOPIK 1-2ê¸‰)",
            "intermediate": "ì¤‘ê¸‰ (TOPIK 3-4ê¸‰)",
            "advanced": "ê³ ê¸‰ (TOPIK 5-6ê¸‰)"
        }
        
        # ê¸°ë³¸ ì •ë³´
        base_info = f"""ã€í•™ìŠµ ì •ë³´ã€‘
ìˆ˜ì¤€: {difficulty_guide.get(difficulty)}
ë¬¸ë²•: {target_grammar} (Grade {target_grade})
ì–´íœ˜: {', '.join(vocab_list)}
"""
        
        # K-pop ì •ë³´
        kpop_info = ""
        if kpop_groups and needs_kpop:
            kpop_info = f"""
ã€K-pop ì •ë³´ã€‘
{kpop_context_text}
ê·¸ë£¹: {', '.join(kpop_groups)}
"""
        
        # ì‹œë„ë³„ í”„ë¡¬í”„íŠ¸
        if attempt == 0:
            # ì²« ì‹œë„: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            prompt = f"""í•œêµ­ì–´ í•™ìŠµìš© ì˜ˆë¬¸ì„ ì •í™•íˆ 3ê°œ ìƒì„±í•˜ì„¸ìš”.

{base_info}{kpop_info}
ã€ìƒì„± ê·œì¹™ã€‘
1. ë¬¸ë²• '{target_grammar}' 3ê°œ ë¬¸ì¥ ëª¨ë‘ì— í•„ìˆ˜ ì‚¬ìš©
2. ì œì‹œ ì–´íœ˜ ì¤‘ ê° ë¬¸ì¥ë§ˆë‹¤ ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨ (ê²¹ì¹˜ì§€ ì•Šê²Œ)
3. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥
4. ë²ˆí˜¸ ì—†ì´ ë¬¸ì¥ 3ê°œë§Œ

ì˜ˆë¬¸:
"""
        
        elif attempt == 1:
            # ë‘ ë²ˆì§¸ ì‹œë„: ê°•í™” + ì´ì „ ì‹¤íŒ¨ ë¶„ì„
            prev = previous_attempts[0]
            failed_items = []
            
            if prev['score'] < 3:
                critique = prev['critique']
                grammar_fail = sum(1 for c in critique if not c.get('grammar_ok', False))
                vocab_fail = sum(1 for c in critique if not c.get('vocab_ok', False))
                
                if grammar_fail > 0:
                    failed_items.append(f"- ë¬¸ë²• '{target_grammar}' ë¯¸í¬í•¨: {grammar_fail}ê°œ ë¬¸ì¥")
                if vocab_fail > 0:
                    failed_items.append(f"- ì–´íœ˜ ë¯¸í¬í•¨: {vocab_fail}ê°œ ë¬¸ì¥")
                if not prev['kpop_ok'] and needs_kpop:
                    failed_items.append(f"- K-pop ì •ë³´ ë¯¸í¬í•¨")
            
            fail_text = "\n".join(failed_items) if failed_items else "ì¼ë¶€ ì¡°ê±´ ë¯¸ì¶©ì¡±"
            
            prompt = f"""âš ï¸ ì´ì „ ì‹œë„ ì‹¤íŒ¨ - ë°˜ë“œì‹œ ëª¨ë“  ì¡°ê±´ì„ ì¶©ì¡±í•˜ì„¸ìš”!

{base_info}{kpop_info}
ã€ì´ì „ ì‹¤íŒ¨ ì›ì¸ã€‘
{fail_text}

ã€í•„ìˆ˜ ì¡°ê±´ã€‘
âœ… ë¬¸ë²• '{target_grammar}' - 3ê°œ ë¬¸ì¥ ëª¨ë‘ ë°˜ë“œì‹œ í¬í•¨!
âœ… ì–´íœ˜ {', '.join(vocab_list)} - ê° ë¬¸ì¥ë§ˆë‹¤ ìµœì†Œ 1ê°œ ê²¹ì¹˜ì§€ ì•Šê²Œ í¬í•¨!
{f"âœ… K-pop '{', '.join(kpop_groups)}' - 3ê°œ ë¬¸ì¥ ëª¨ë‘ í¬í•¨!" if needs_kpop and kpop_groups else ""}

ã€ìƒì„± ê·œì¹™ã€‘
1. ë¬¸ë²• íŒ¨í„´ì„ ëª…í™•í•˜ê²Œ ì‚¬ìš©
2. ê° ë¬¸ì¥ë§ˆë‹¤ ë‹¤ë¥¸ ì–´íœ˜ ì‚¬ìš©
3. ìì—°ìŠ¤ëŸ½ê³  ì‹¤ìš©ì ì¸ ë¬¸ì¥

ì˜ˆë¬¸:
"""
        
        else:  # attempt == 2
            # ì„¸ ë²ˆì§¸ ì‹œë„: ìµœëŒ€ ê°•í™” + ëª…í™•í•œ ì–´íœ˜ í• ë‹¹
            vocab_assignment = ""
            for i, word in enumerate(vocab_list[:3], 1):
                vocab_assignment += f"   ë¬¸ì¥{i}: '{word}' ë°˜ë“œì‹œ í¬í•¨\n"
            
            prompt = f"""ğŸš¨ ìµœì¢… ì‹œë„ - ì•„ë˜ ì§€ì‹œì‚¬í•­ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”!

{base_info}{kpop_info}
ã€ëª…í™•í•œ ì–´íœ˜ í• ë‹¹ã€‘
{vocab_assignment}

ã€ì ˆëŒ€ ê·œì¹™ã€‘
1. ë¬¸ë²• '{target_grammar}' - 3ê°œ ë¬¸ì¥ ëª¨ë‘ ëª…í™•í•˜ê²Œ ì‚¬ìš©
2. ìœ„ ì–´íœ˜ í• ë‹¹í‘œëŒ€ë¡œ ê° ë¬¸ì¥ì— ì§€ì •ëœ ì–´íœ˜ ë°˜ë“œì‹œ í¬í•¨
3. ìì—°ìŠ¤ëŸ½ê³  ë¬¸ë²•ì ìœ¼ë¡œ ì™„ë²½í•œ ë¬¸ì¥
{f"4. K-pop ê·¸ë£¹ '{', '.join(kpop_groups)}' ë°˜ë“œì‹œ í¬í•¨ (ì˜ì–´â†’í•œê¸€)" if needs_kpop and kpop_groups else ""}

ã€ì˜ˆì‹œ í˜•ì‹ã€‘
ë¬¸ì¥1: [ì–´íœ˜1 + ë¬¸ë²• + K-pop]
ë¬¸ì¥2: [ì–´íœ˜2 + ë¬¸ë²• + K-pop]
ë¬¸ì¥3: [ì–´íœ˜3 + ë¬¸ë²• + K-pop]

ì˜ˆë¬¸:
"""
        
        return prompt
    
    def _process_kpop_docs(self, kpop_docs, specified_groups):
        """K-pop ë¬¸ì„œ ì²˜ë¦¬ ë° í•„í„°ë§"""
        kpop_metadata = []
        kpop_context_text = ""
        kpop_groups = []
        
        if not kpop_docs:
            return kpop_metadata, kpop_context_text, kpop_groups
        
        # í•„í„°ë§
        filtered_docs = kpop_docs[:3]
        if specified_groups:
            filtered = []
            for doc in kpop_docs:
                group = doc.metadata.get('group', '')
                if any(g.upper() == group.upper() for g in specified_groups):
                    filtered.append(doc)
            
            if filtered:
                filtered_docs = filtered[:3]
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        for doc in filtered_docs:
            group = doc.metadata.get('group', '')
            if group:
                kpop_groups.append(group)
                
                meta = {
                    "group": group,
                    "agency": doc.metadata.get('agency', ''),
                    "fandom": doc.metadata.get('fandom', ''),
                    "concepts": doc.metadata.get('concepts', []),
                    "members": [m.get("name", "") for m in doc.metadata.get('members', [])[:4]]
                }
                kpop_metadata.append(meta)
                
                # ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸
                kpop_context_text += f"ã€{group}ã€‘\n"
                if meta['agency']:
                    kpop_context_text += f"  ì†Œì†ì‚¬: {meta['agency']}\n"
                if meta['fandom']:
                    kpop_context_text += f"  íŒ¬ë¤: {meta['fandom']}\n"
                if meta['concepts']:
                    kpop_context_text += f"  ì»¨ì…‰: {', '.join(meta['concepts'])}\n"
                if meta['members']:
                    kpop_context_text += f"  ë©¤ë²„: {', '.join(meta['members'])}\n"
                kpop_context_text += "\n"
        
        return kpop_metadata, kpop_context_text, kpop_groups
    
    def _check_kpop_inclusion(self, sentences, kpop_groups):
        """K-pop ê·¸ë£¹ëª… í¬í•¨ ì—¬ë¶€ ì²´í¬"""
        if not kpop_groups:
            return True
        
        # ì˜ì–´ ê·¸ë£¹ëª…ì˜ í•œê¸€ ë³€í™˜ ë§¤í•‘
        korean_names = {
            "BLACKPINK": "ë¸”ë™í•‘í¬",
            "BTS": "ë°©íƒ„ì†Œë…„ë‹¨",
            "TWICE": "íŠ¸ì™€ì´ìŠ¤",
            "NewJeans": "ë‰´ì§„ìŠ¤",
            "EXO": "ì—‘ì†Œ",
            "Stray Kids": "ìŠ¤íŠ¸ë ˆì´í‚¤ì¦ˆ",
            "aespa": "ì—ìŠ¤íŒŒ",
            "SEVENTEEN": "ì„¸ë¸í‹´"
        }
        
        # ëª¨ë“  ë¬¸ì¥ì—ì„œ K-pop ì •ë³´ í¬í•¨ í™•ì¸
        for sentence in sentences:
            has_kpop = False
            for group in kpop_groups:
                # ì˜ì–´ëª… ë˜ëŠ” í•œê¸€ëª… ì²´í¬
                if (group.lower() in sentence.lower() or 
                    korean_names.get(group, "").lower() in sentence.lower()):
                    has_kpop = True
                    break
            
            if not has_kpop:
                return False
        
        return True

    def format_output_agentic(self, state: GraphState) -> GraphState:
        """Agentic RAG ì¶œë ¥ í¬ë§·íŒ…"""
        print("\nğŸ“„ [Agent] ìµœì¢… ì¶œë ¥")
        
        output = "=" * 80 + "\n"
        output += "ğŸ“ í•œêµ­ì–´ í•™ìŠµ ë¬¸ì œ ìƒì„± (Agentic RAG)\n"
        output += "=" * 80 + "\n\n"
        
        # ìƒì„±ëœ ë¬¸ì¥
        sentences = state.get('generated_sentences', [])
        if sentences:
            output += "ã€ìƒì„±ëœ í•™ìŠµ ì˜ˆë¬¸ã€‘\n"
            for i, sentence in enumerate(sentences, 1):
                output += f"   {i}. {sentence}\n"
        
        # íŒŒì¼ ì €ì¥
        if 'sentence_data' in state and state['sentence_data']:
            saved_file = self._save_to_json(state['sentence_data'])
            output += f"\nğŸ’¾ ì €ì¥: {saved_file}\n"
        
        output += "\n" + "=" * 80 + "\n"
        
        return {"final_output": output}