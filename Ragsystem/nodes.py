# =====================================
# nodes.py (Updated) - Evaluator í†µí•© ë²„ì „
# ìˆ˜ì • ì™„ë£Œ
# =====================================
"""
LangGraph ë…¸ë“œ ì •ì˜ (ë¬¸ì¥ ì €ì¥ ê¸°ëŠ¥ ë° í‰ê°€ ê¸°ëŠ¥ í¬í•¨)
"""
import json
import os
import re
import random
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime
from langchain.chat_models import ChatOpenAI
from Ragsystem.schema import GraphState
from utils import (
    detect_difficulty_from_text,
    extract_words_from_docs,
    extract_grammar_with_grade  
)
from config import LLM_CONFIG, SENTENCE_SAVE_DIR
from agents import QueryAnalysisAgent, QualityCheckAgent

# Evaluator ì„í¬íŠ¸ (optional)
try:
    from Evaluator.kpop_evaluator import KpopSentenceEvaluator
    EVALUATOR_ENABLED = True
except ImportError:
    EVALUATOR_ENABLED = False
    print("âš ï¸ Evaluator ëª¨ë“ˆ ì—†ìŒ - ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰")

INVALID_CHARS = r'[<>:"/\\|?*\x00-\x1F]'

def sanitize_filename(name: str, replacement: str = "_") -> str:
    """Windows íŒŒì¼ëª… ì•ˆì „ ì²˜ë¦¬"""
    safe = re.sub(INVALID_CHARS, replacement, name)
    safe = safe.strip().strip(".")
    RESERVED = {"CON","PRN","AUX","NUL",*(f"COM{i}" for i in range(1,10)),*(f"LPT{i}" for i in range(1,10))}
    if safe.upper() in RESERVED:
        safe = f"_{safe}"
    return safe[:120] if len(safe) > 120 else safe

class KoreanLearningNodes:
    """í•œêµ­ì–´ í•™ìŠµ ë…¸ë“œ í´ë˜ìŠ¤"""
    
    def __init__(self, vocabulary_retriever, grammar_retriever, kpop_retriever, llm=None):
        self.vocabulary_retriever = vocabulary_retriever
        self.grammar_retriever = grammar_retriever
        self.kpop_retriever = kpop_retriever
        self.llm = llm or ChatOpenAI(
            model="gpt-4o-mini",
            temperature=LLM_CONFIG.get('temperature', 0.7),
            max_tokens=LLM_CONFIG.get('max_tokens', 1000)
        )
        
        # Evaluator ì´ˆê¸°í™”
        self.evaluator = None
        if EVALUATOR_ENABLED:
            try:
                self.evaluator = KpopSentenceEvaluator()
                print("   âœ… ë¬¸ì¥ í‰ê°€ê¸° í™œì„±í™”")
            except Exception as e:
                print(f"   â„¹ï¸ í‰ê°€ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.evaluator = None
        
        # sentence í´ë” ìƒì„±
        self.output_dir = "sentence"
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def detect_difficulty(self, state: GraphState) -> GraphState:
        """ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ ë‚œì´ë„ ê°ì§€"""
        difficulty = detect_difficulty_from_text(state['input_text'])
        return {"difficulty_level": difficulty}
    
    def retrieve_vocabulary(self, state: GraphState) -> GraphState:
        """ë‹¨ì–´ ê²€ìƒ‰ ë…¸ë“œ"""
        level = state['difficulty_level']
        query = state['input_text']
        vocab_docs = self.vocabulary_retriever.invoke(query, level)
        return {"vocabulary_docs": vocab_docs}

    def retrieve_kpop(self, state: GraphState) -> GraphState:
        """K-pop ë¬¸ì¥ ê²€ìƒ‰ ë…¸ë“œ"""
        level = state['difficulty_level']
        query = state['input_text']
        kpop_docs = self.kpop_retriever.invoke(query, level)
        return {"kpop_docs": kpop_docs}
    
    def retrieve_grammar(self, state: GraphState) -> GraphState:
        """ë¬¸ë²• ê²€ìƒ‰ ë…¸ë“œ"""
        level = state['difficulty_level']
        query = state['input_text']
        grammar_docs = self.grammar_retriever.invoke(query, level)
        return {"grammar_docs": grammar_docs}

    def generate_sentences(self, state: GraphState) -> GraphState:
        """ê²€ìƒ‰ëœ ë‹¨ì–´ì™€ ë¬¸ë²•ì„ í™œìš©í•œ ë¬¸ì¥ ìƒì„± (K-pop ì •ë³´ í¬í•¨)"""
        words_info = extract_words_from_docs(state['vocabulary_docs'])
        
        # K-pop ì •ë³´ ì¶”ì¶œ
        kpop_references = []
        kpop_context_text = ""
        
        if 'kpop_docs' in state and state['kpop_docs']:
            print(f"[ì°¸ì¡°] K-pop ë¬¸ì„œ ê°œìˆ˜: {len(state['kpop_docs'])}")
            
            for doc in state['kpop_docs'][:3]:
                sentence = doc.metadata.get('sentence', '')
                song = doc.metadata.get('song', '')
                group = doc.metadata.get('group', '')
                
                if sentence:
                    kpop_references.append({
                        "sentence": sentence,
                        "song": song,
                        "group": group,
                    })
                    kpop_context_text += f'- "{sentence}" ({song} - {group})\n'
        
        print(f"[ì°¸ì¡°] K-pop ì°¸ì¡° ê°œìˆ˜: {len(kpop_references)}")
        
        # ë¬¸ë²•ê³¼ grade ì •ë³´ ì¶”ì¶œ
        grammar_info = extract_grammar_with_grade(state['grammar_docs'])
        
        # ì–´íœ˜ í¬ë§·íŒ…
        words_formatted = []
        vocab_list = []  # í‰ê°€ìš©
        for word, wordclass in words_info[:5]:
            words_formatted.append(f"{word}({wordclass})")
            vocab_list.append(word)
        
        if grammar_info:
            random_grammar_item = random.choice(grammar_info)
            target_grammar = random_grammar_item['grammar']
            target_grade = random_grammar_item['grade']
            print("grammar : ", target_grammar)
            print("grade : ", target_grade)
        else:
            target_grammar = "ê¸°ë³¸ ë¬¸ë²•"
            target_grade = 1
        
        # ë‚œì´ë„ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        difficulty = state['difficulty_level']
        difficulty_guide = {
            "basic": "ì´ˆê¸‰ í•™ìŠµì (TOPIK 1-2ê¸‰): ì§§ê³  ê°„ë‹¨í•œ ë¬¸ì¥, ê¸°ë³¸ ì‹œì œ ì‚¬ìš©",
            "intermediate": "ì¤‘ê¸‰ í•™ìŠµì (TOPIK 3-4ê¸‰): ë‹¤ì–‘í•œ ì—°ê²°ì–´ë¯¸, ìì—°ìŠ¤ëŸ¬ìš´ ì¼ìƒ ëŒ€í™” í‘œí˜„",
            "advanced": "ê³ ê¸‰ í•™ìŠµì (TOPIK 5-6ê¸‰): ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°, ê²©ì‹ì²´ë‚˜ ë¬¸ì–´ì²´ ê°€ëŠ¥"
        }
        
        prompt = self._build_generation_prompt(
            difficulty, 
            target_grade, 
            words_formatted, 
            target_grammar, 
            kpop_context_text,
            difficulty_guide
        )
        
        # ë¬¸ì¥ ìƒì„± (3ê°œ)
        response = self.llm.predict(prompt)
        sentences = response.strip().split('\n')
        sentences = [s.strip() for s in sentences if s.strip()][:3]
        
        # í‰ê°€ ìˆ˜í–‰ (ìˆì„ ê²½ìš°)
        critique_summary = self._evaluate_sentences(
            sentences, 
            target_grammar, 
            vocab_list
        )
        
        # JSON ì €ì¥ ë°ì´í„°
        save_data = {
            "level": target_grade,
            "target_grammar": target_grammar,
            "kpop_references": kpop_references,
            "critique_summary": critique_summary
        }
        
        messages = [
            ("user", state['input_text']),
            ("assistant", "\n".join(sentences))
        ]
        
        return {
            "generated_sentences": sentences,
            "messages": messages,
            "sentence_data": save_data,
            "target_grade": target_grade
        }
    
    def _build_generation_prompt(self, difficulty, target_grade, words_formatted, 
                                target_grammar, kpop_context_text, difficulty_guide):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± (ë‚œì´ë„ë³„)"""
        prompt_templates = {
            "basic": """
[ROLE]
ë„ˆëŠ” í•œêµ­ì–´ë¥¼ ë°°ìš°ëŠ” ì´ˆê¸‰ í•™ìŠµìë¥¼ ìœ„í•œ ì¹œì ˆí•œ í•œêµ­ì–´ ì„ ìƒë‹˜ì´ì•¼.

[INSTRUCTIONS]
- í•™ìŠµ ìˆ˜ì¤€: {difficulty_level} (Grade {target_grade})
- ì˜¤ëŠ˜ì˜ ë‹¨ì–´: {words_formatted}
- ì˜¤ëŠ˜ì˜ ë¬¸ë²•: {target_grammar}
- K-pop ì°¸ê³ : {kpop_context_text}

[SENTENCE RULES]
1. ì§§ê³  ê°„ë‹¨í•œ ë¬¸ì¥ (10-15 ë‹¨ì–´)
2. ê¸°ë³¸ ì‹œì œë§Œ ì‚¬ìš©
3. ë¬¸ë²• íŒ¨í„´ {target_grammar} í•„ìˆ˜ í¬í•¨
4. ì œì‹œëœ ë‹¨ì–´ ìµœì†Œ 3ê°œ í¬í•¨

í˜•ì‹: ë²ˆí˜¸ ì—†ì´ ë¬¸ì¥ 3ê°œë§Œ
""",
            "intermediate": """
[ROLE]
ë„ˆëŠ” ì¤‘ê¸‰ í•œêµ­ì–´ í•™ìŠµìë¥¼ ìœ„í•œ ê²½í—˜ ë§ì€ í•œêµ­ì–´ êµì‚¬ì•¼.

[INSTRUCTIONS]
- í•™ìŠµ ìˆ˜ì¤€: {difficulty_level} (Grade {target_grade})
- í•µì‹¬ ì–´íœ˜: {words_formatted}
- ëª©í‘œ ë¬¸ë²•: {target_grammar}
- K-pop ì°¸ê³ : {kpop_context_text}

[REQUIREMENTS]
1. ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´
2. ë¬¸ë²• {target_grammar} í™œìš©
3. ì œì‹œëœ ì–´íœ˜ 3-4ê°œ í¬í•¨
4. ì‹¤ìƒí™œ ìƒí™© ë°˜ì˜

ì¶œë ¥: ì˜ˆë¬¸ 3ê°œë§Œ (ë²ˆí˜¸ ì—†ì´)
""",
            "advanced": """
[ROLE]
ë„ˆëŠ” ê³ ê¸‰ í•œêµ­ì–´ í•™ìŠµìë¥¼ ìœ„í•œ ì „ë¬¸ êµìˆ˜ë‹¤.

[INSTRUCTIONS]
- í•™ìŠµ ìˆ˜ì¤€: {difficulty_level} (Grade {target_grade})
- í•µì‹¬ ì–´íœ˜: {words_formatted}
- í•µì‹¬ ë¬¸ë²•: {target_grammar}
- K-pop ì°¸ê³ : {kpop_context_text}

[REQUIREMENTS]
1. ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°
2. ë¬¸ë²• {target_grammar} ì‹¬í™” í™œìš©
3. ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš©
4. ë¬¸ì–´ì²´ ë˜ëŠ” ê²©ì‹ì²´

ì¶œë ¥: ì˜ˆë¬¸ 3ê°œë§Œ
"""
        }
        
        template = prompt_templates.get(difficulty, prompt_templates["intermediate"])
        return template.format(
            difficulty_level=difficulty_guide.get(difficulty, difficulty),
            target_grade=target_grade,
            words_formatted=', '.join(words_formatted),
            target_grammar=target_grammar,
            kpop_context_text=kpop_context_text if kpop_context_text else "ì—†ìŒ"
        )
    
    def _evaluate_sentences(self, sentences, target_grammar, vocab_list):
        """ìƒì„±ëœ ë¬¸ì¥ í‰ê°€"""
        if self.evaluator and sentences:
            try:
                print("\n   ğŸ“Š ìƒì„±ëœ ë¬¸ì¥ í’ˆì§ˆ í‰ê°€ ì¤‘...")
                evaluation_results = self.evaluator.evaluate_batch(
                    sentences,
                    grammar=target_grammar,
                    vocab=vocab_list
                )
                
                # í‰ê°€ ê²°ê³¼ë¥¼ critique_summaryì— í¬í•¨
                critique_summary = []
                for sent, eval_res in zip(sentences, evaluation_results):
                    critique_summary.append({
                        "sentence": sent,
                        "grammar_ok": eval_res.get("grammar_ok", False),
                        "vocab_ok": eval_res.get("vocab_ok", False)
                    })
                
                return critique_summary
                
            except Exception as e:
                print(f"   âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # í‰ê°€ ì—†ì´ ê¸°ë³¸ í˜•ì‹
        return [{"sentence": s} for s in sentences]
    
    def format_output(self, state: GraphState) -> GraphState:
        """ìµœì¢… ì¶œë ¥ í¬ë§·íŒ… ë° JSON ì €ì¥"""
        output = f"=== í•œêµ­ì–´ í•™ìŠµ ë¬¸ì œ ìƒì„± ê²°ê³¼ ===\n"
        output += f"ë‚œì´ë„: {state['difficulty_level']}\n"
        
        if 'target_grade' in state:
            output += f"ë¬¸ë²• Grade: {state['target_grade']}\n"
        
        output += "\nìƒì„±ëœ ì˜ˆë¬¸:\n"
        for i, sentence in enumerate(state['generated_sentences'], 1):
            output += f"{i}. {sentence}\n"
        
        # JSON íŒŒì¼ ì €ì¥
        if 'sentence_data' in state and state['sentence_data']:
            saved_file = self._save_to_json(state['sentence_data'])
            output += f"\nì˜ˆë¬¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {saved_file}\n"
        
        return {"final_output": output}
    
    def _save_to_json(self, sentence_data: dict) -> str:
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        out_dir = Path(SENTENCE_SAVE_DIR)
        out_dir.mkdir(parents=True, exist_ok=True)

        level = sentence_data.get("level", "grade1")
        title = sentence_data.get("title", "untitled")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        base = f"sentences_{level}_{title}_{timestamp}"
        safe_base = sanitize_filename(base)
        filepath = out_dir / f"{safe_base}.json"

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(sentence_data, f, ensure_ascii=False, indent=2)

        return str(filepath)


# Agentic RAG êµ¬í˜„
class AgenticKoreanLearningNodes(KoreanLearningNodes):
    """
    Agentic RAG ë…¸ë“œ (KoreanLearningNodes ìƒì†)
    """
    
    def __init__(self, vocabulary_retriever, grammar_retriever, kpop_retriever, llm=None):
        super().__init__(vocabulary_retriever, grammar_retriever, kpop_retriever, llm)
        
        # Agentic ì—ì´ì „íŠ¸ ì¶”ê°€
        self.query_agent = QueryAnalysisAgent(llm)
        self.quality_agent = QualityCheckAgent(llm)
    
    def analyze_query_agent(self, state: GraphState) -> GraphState:
        """ì¿¼ë¦¬ ë¶„ì„ ì—ì´ì „íŠ¸ ë…¸ë“œ"""
        print("\nğŸ” [Agent] Query Analysis")
        analysis = self.query_agent.analyze(state['input_text'])
        
        print(f"   Difficulty: {analysis['difficulty']}")
        print(f"   Topic: {analysis['topic']}")
        print(f"   Needs K-pop: {analysis.get('needs_kpop', False)}")
        print(f"   K-pop Groups: {analysis.get('kpop_groups', [])}")
        
        return {
            "difficulty_level": analysis['difficulty'],
            "query_analysis": analysis
        }
    
    def retrieve_kpop_mixed(self, state: GraphState) -> GraphState:
        """K-pop ê²€ìƒ‰ ë…¸ë“œ (DB ì „ìš©)"""
        print("\nğŸµ [Agent] K-pop Retrieval (DB Only)")
        
        level = state['difficulty_level']
        query = state['input_text']
        
        kpop_db_docs = self.kpop_retriever.invoke(query, level)
        kpop_db_docs = kpop_db_docs[:5]
        print(f"   DB ê²€ìƒ‰: {len(kpop_db_docs)}ê°œ")
        
        return {"kpop_docs": kpop_db_docs}
    
    def check_quality_agent(self, state: GraphState) -> GraphState:
        """í’ˆì§ˆ ê²€ì¦ ì—ì´ì „íŠ¸ ë…¸ë“œ"""
        print("\nâœ… [Agent] í’ˆì§ˆ ê²€ì¦")
        
        query_analysis = state.get('query_analysis', {})
        needs_kpop = query_analysis.get('needs_kpop', False)
        
        result = self.quality_agent.check(
            vocab_count=len(state.get('vocabulary_docs', [])),
            grammar_count=len(state.get('grammar_docs', [])),
            kpop_db_count=len(state.get('kpop_docs', [])),
            needs_kpop=needs_kpop
        )
        
        print(f"   ì–´íœ˜: {result['vocab_count']}ê°œ")
        print(f"   ë¬¸ë²•: {result['grammar_count']}ê°œ")
        print(f"   K-pop: {result['kpop_db_count']}ê°œ")
        print(f"   ìƒíƒœ: {result['message']}")
        
        return {"quality_check": result}
    
    def generate_sentences_with_kpop(self, state):
        """
        K-pop ë©”íƒ€ë°ì´í„°ë¥¼ í™œìš©í•œ í•œêµ­ì–´ í•™ìŠµ ë¬¸ì¥ ìƒì„±
        3ê°œ ìƒì„± â†’ í‰ê°€ ìˆ˜í–‰
        """
        print("\nâœï¸ [Agent] í•œêµ­ì–´ í•™ìŠµ ë¬¸ì¥ ìƒì„± (K-pop í†µí•©)")
        
        from utils import extract_words_from_docs, extract_grammar_with_grade
        
        words_info = extract_words_from_docs(state['vocabulary_docs'])
        grammar_info = extract_grammar_with_grade(state['grammar_docs'])
        
        # ì¿¼ë¦¬ ë¶„ì„ ì •ë³´
        query_analysis = state.get('query_analysis', {})
        needs_kpop = query_analysis.get('needs_kpop', False)
        specified_groups = query_analysis.get('kpop_groups', [])
        
        print(f"   ì¿¼ë¦¬ ë¶„ì„: needs_kpop={needs_kpop}, ì§€ì • ê·¸ë£¹={specified_groups}")
        
        # K-pop ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
        kpop_metadata, kpop_context_text, kpop_groups = self._process_kpop_docs(
            state.get('kpop_docs', []),
            specified_groups
        )
        
        has_kpop = len(kpop_metadata) > 0
        
        if has_kpop:
            print(f"   K-pop ì •ë³´: {len(kpop_metadata)}ê°œ - {kpop_groups}")
        else:
            print(f"   K-pop ì •ë³´: ì—†ìŒ")
        
        # ì–´íœ˜/ë¬¸ë²• ì¤€ë¹„
        words_formatted = []
        vocab_list = []  # í‰ê°€ìš©
        for word, wordclass in words_info[:5]:
            words_formatted.append(f"{word}({wordclass})")
            vocab_list.append(word)
        
        if grammar_info:
            random_grammar_item = random.choice(grammar_info)
            target_grammar = random_grammar_item['grammar']
            target_grade = random_grammar_item['grade']
        else:
            target_grammar = "ê¸°ë³¸ ë¬¸ë²•"
            target_grade = 1
        
        difficulty = state['difficulty_level']
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_kpop_prompt(
            difficulty,
            target_grade,
            target_grammar,
            words_formatted,
            has_kpop,
            needs_kpop,
            kpop_context_text,
            kpop_groups
        )
        
        print(f"\n   ğŸ¯ íƒ€ê²Ÿ: ë¬¸ë²• '{target_grammar}' + ì–´íœ˜ {len(words_formatted)}ê°œ")
        
        # ë¬¸ì¥ ìƒì„± (3ê°œ)
        response = self.llm.predict(prompt)
        sentences = response.strip().split('\n')
        sentences = [s.strip() for s in sentences if s.strip()][:3]
        
        print(f"   âœ… {len(sentences)}ê°œ ë¬¸ì¥ ìƒì„± ì™„ë£Œ")
        
        # í‰ê°€ ìˆ˜í–‰
        critique_summary = self._evaluate_sentences(
            sentences,
            target_grammar,
            vocab_list
        )
        
        # JSON ì €ì¥ ë°ì´í„°
        save_data = {
            "level": target_grade,
            "target_grammar": target_grammar,
            "kpop_references": kpop_metadata,
            "specified_groups": specified_groups,
            "critique_summary": critique_summary
        }
        
        messages = [
            ("user", state['input_text']),
            ("assistant", "\n".join(sentences))
        ]
        
        return {
            "generated_sentences": sentences,
            "messages": messages,
            "sentence_data": save_data,
            "target_grade": target_grade
        }
    
    def _process_kpop_docs(self, kpop_docs, specified_groups):
        """K-pop ë¬¸ì„œ ì²˜ë¦¬ ë° í•„í„°ë§"""
        kpop_metadata = []
        kpop_context_text = ""
        kpop_groups = []
        
        if not kpop_docs:
            return kpop_metadata, kpop_context_text, kpop_groups
        
        # í•„í„°ë§
        filtered_docs = []
        if specified_groups:
            for doc in kpop_docs:
                group = doc.metadata.get('group', '')
                if any(g.upper() == group.upper() for g in specified_groups):
                    filtered_docs.append(doc)
            
            if not filtered_docs:
                filtered_docs = kpop_docs[:3]
        else:
            filtered_docs = kpop_docs[:3]
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        for doc in filtered_docs:
            group = doc.metadata.get('group', '')
            if group:
                kpop_groups.append(group)
                
                meta = {
                    "group": group,
                    "agency": doc.metadata.get('agency', ''),
                    "fandom": doc.metadata.get('fandom', ''),
                    "concepts": doc.metadata.get('concepts', []),
                    "members": [m.get("name", "") for m in doc.metadata.get('members', [])[:4]]
                }
                kpop_metadata.append(meta)
                
                # ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸
                kpop_context_text += f"ã€{group}ã€‘\n"
                if meta['agency']:
                    kpop_context_text += f"  ì†Œì†ì‚¬: {meta['agency']}\n"
                if meta['fandom']:
                    kpop_context_text += f"  íŒ¬ë¤: {meta['fandom']}\n"
                if meta['concepts']:
                    kpop_context_text += f"  ì»¨ì…‰: {', '.join(meta['concepts'])}\n"
                if meta['members']:
                    kpop_context_text += f"  ë©¤ë²„: {', '.join(meta['members'])}\n"
                kpop_context_text += "\n"
        
        return kpop_metadata, kpop_context_text, kpop_groups
    
    def _build_kpop_prompt(self, difficulty, target_grade, target_grammar, 
                          words_formatted, has_kpop, needs_kpop, 
                          kpop_context_text, kpop_groups):
        """K-pop í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        difficulty_guide = {
            "basic": "ì´ˆê¸‰ (TOPIK 1-2ê¸‰): ì§§ê³  ê°„ë‹¨í•œ ë¬¸ì¥",
            "intermediate": "ì¤‘ê¸‰ (TOPIK 3-4ê¸‰): ìì—°ìŠ¤ëŸ¬ìš´ ì¼ìƒ í‘œí˜„",
            "advanced": "ê³ ê¸‰ (TOPIK 5-6ê¸‰): ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°"
        }
        
        # K-pop ì§€ì‹œì‚¬í•­
        kpop_instruction = ""
        kpop_requirement = ""
        
        if has_kpop and needs_kpop:
            groups_text = ', '.join(kpop_groups)
            kpop_instruction = f"""
ã€K-pop ê·¸ë£¹ ì •ë³´ã€‘
{kpop_context_text}

âš ï¸ K-pop í•„ìˆ˜ ê·œì¹™:
- ìœ„ ê·¸ë£¹({groups_text})ë§Œ ì‚¬ìš©
- ì˜ì–´ëŠ” í•œêµ­ì–´ë¡œ: "BLACKPINK"â†’"ë¸”ë™í•‘í¬"
- 3ê°œ ë¬¸ì¥ ëª¨ë‘ K-pop í¬í•¨
"""
            kpop_requirement = f"í•„ìˆ˜: {groups_text} ë‚´ìš© í¬í•¨"
            
        elif has_kpop:
            groups_text = ', '.join(kpop_groups)
            kpop_instruction = f"""
ã€K-pop ì •ë³´ (ì„ íƒ)ã€‘
{kpop_context_text}
"""
            kpop_requirement = f"ì„ íƒ: {groups_text} í™œìš© ê°€ëŠ¥"
        
        prompt = f"""í•œêµ­ì–´ í•™ìŠµìš© ì˜ˆë¬¸ì„ ì •í™•íˆ 3ê°œ ìƒì„±í•˜ì„¸ìš”.

ã€í•™ìŠµ ì •ë³´ã€‘
- ìˆ˜ì¤€: {difficulty_guide.get(difficulty)}
- ë¬¸ë²•: {target_grammar} (Grade {target_grade})
- ì–´íœ˜: {', '.join(words_formatted)}
{kpop_instruction}
ã€ìƒì„± ê·œì¹™ã€‘
1. ë¬¸ë²• '{target_grammar}' í•„ìˆ˜ ì‚¬ìš©
2. ì œì‹œ ì–´íœ˜ ì¤‘ 3ê°œ ì´ìƒ í¬í•¨
3. ë¬¸ì¥ 3ê°œë§Œ, ë²ˆí˜¸ ì—†ì´
{f'4. {kpop_requirement}' if kpop_requirement else ''}

ì˜ˆë¬¸:
"""
        return prompt
    
    def format_output_agentic(self, state: GraphState) -> GraphState:
        """Agentic RAG ì¶œë ¥ í¬ë§·íŒ…"""
        print("\nğŸ“„ [Agent] ìµœì¢… ì¶œë ¥")
        
        output = "=" * 80 + "\n"
        output += "ğŸ“ í•œêµ­ì–´ í•™ìŠµ ë¬¸ì œ ìƒì„± (Agentic RAG)\n"
        output += "=" * 80 + "\n\n"
        
        # ìƒì„±ëœ ë¬¸ì¥
        sentences = state.get('generated_sentences', [])
        if sentences:
            output += "ã€ìƒì„±ëœ í•™ìŠµ ì˜ˆë¬¸ã€‘\n"
            for i, sentence in enumerate(sentences, 1):
                output += f"   {i}. {sentence}\n"
        
        # íŒŒì¼ ì €ì¥
        if 'sentence_data' in state and state['sentence_data']:
            saved_file = self._save_to_json(state['sentence_data'])
            output += f"\nğŸ’¾ ì €ì¥: {saved_file}\n"
        
        output += "\n" + "=" * 80 + "\n"
        
        return {"final_output": output}