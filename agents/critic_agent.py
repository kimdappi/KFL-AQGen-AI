"""
critic_agent.py - ìƒì„±ëœ ë¬¸ì œì˜ í’ˆì§ˆê³¼ ì ì ˆì„±ì„ ê²€ì¦í•˜ëŠ” ì—ì´ì „íŠ¸
"""
from base_agent import BaseAgent
from typing import Dict, Any, List
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class CriticAgent(BaseAgent):
    """ë¬¸ì œ í’ˆì§ˆ ê²€ì¦ ì—ì´ì „íŠ¸"""
    
    def __init__(self, model_name: str = None):
        super().__init__(model_name, agent_name="CriticAgent")
        
        # í‰ê°€ ê¸°ì¤€
        self.evaluation_criteria = {
            'difficulty_match': {
                'weight': 0.25,
                'description': 'ë‚œì´ë„ ì ì ˆì„±'
            },
            'content_relevance': {
                'weight': 0.25,
                'description': 'ê´€ì‹¬ì‚¬ ê´€ë ¨ì„±'
            },
            'linguistic_quality': {
                'weight': 0.20,
                'description': 'ì–¸ì–´ì  í’ˆì§ˆ'
            },
            'educational_value': {
                'weight': 0.20,
                'description': 'êµìœ¡ì  ê°€ì¹˜'
            },
            'format_correctness': {
                'weight': 0.10,
                'description': 'í˜•ì‹ ì •í™•ì„±'
            }
        }
        
        # ë‚œì´ë„ë³„ ê¸°ì¤€
        self.difficulty_standards = {
            'beginner': {
                'vocab_count': 500,  # ê¸°ì´ˆ ì–´íœ˜ ìˆ˜
                'sentence_complexity': 'simple',  # ë‹¨ë¬¸ ìœ„ì£¼
                'grammar_patterns': ['present', 'past', 'basic_particles'],
                'max_sentence_length': 30
            },
            'intermediate': {
                'vocab_count': 2000,
                'sentence_complexity': 'moderate',  # ë³µë¬¸ í¬í•¨
                'grammar_patterns': ['all_tenses', 'conjunctions', 'honorifics'],
                'max_sentence_length': 50
            },
            'advanced': {
                'vocab_count': 5000,
                'sentence_complexity': 'complex',  # ë³µì¡í•œ êµ¬ì¡°
                'grammar_patterns': ['passive', 'causative', 'idioms', 'proverbs'],
                'max_sentence_length': 70
            }
        }
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë¬¸ì œ ê²€ì¦ ì²˜ë¦¬
        
        Args:
            input_data: {
                'questions': List[Dict],  # ê²€ì¦í•  ë¬¸ì œë“¤
                'difficulty': str,  # ëª©í‘œ ë‚œì´ë„
                'interest': str,  # ê´€ì‹¬ì‚¬
                'age_group': str,  # ë‚˜ì´ëŒ€
                'content': List[str]  # ì›ë³¸ ì½˜í…ì¸ 
            }
        
        Returns:
            {
                'evaluation': Dict,  # í‰ê°€ ê²°ê³¼
                'approved_questions': List[Dict],  # ìŠ¹ì¸ëœ ë¬¸ì œ
                'rejected_questions': List[Dict],  # ê±°ë¶€ëœ ë¬¸ì œ
                'suggestions': List[str],  # ê°œì„  ì œì•ˆ
                'overall_score': float  # ì „ì²´ ì ìˆ˜
            }
        """
        if not self.validate_input(input_data, ['questions', 'difficulty', 'interest']):
            return {'error': 'í•„ìˆ˜ ì…ë ¥ ëˆ„ë½'}
        
        questions = input_data['questions']
        difficulty = input_data['difficulty']
        interest = input_data['interest']
        age_group = input_data.get('age_group', '20ëŒ€')
        content = input_data.get('content', [])
        
        logger.info(f"ğŸ” {len(questions)}ê°œ ë¬¸ì œ ê²€ì¦ ì‹œì‘...")
        
        # 1. ê°œë³„ ë¬¸ì œ í‰ê°€
        evaluated_questions = []
        for q in questions:
            evaluation = self._evaluate_question(q, difficulty, interest, age_group)
            evaluated_questions.append({
                'question': q,
                'evaluation': evaluation,
                'score': evaluation['total_score']
            })
        
        # 2. ë¬¸ì œ ë¶„ë¥˜ (ìŠ¹ì¸/ê±°ë¶€)
        threshold = 0.6  # 60ì  ì´ìƒ ìŠ¹ì¸
        approved = [eq for eq in evaluated_questions if eq['score'] >= threshold]
        rejected = [eq for eq in evaluated_questions if eq['score'] < threshold]
        
        # 3. ì „ì²´ í‰ê°€
        overall_evaluation = self._evaluate_overall(
            evaluated_questions, difficulty, interest
        )
        
        # 4. ê°œì„  ì œì•ˆ ìƒì„±
        suggestions = self._generate_suggestions(
            evaluated_questions, difficulty, interest, overall_evaluation
        )
        
        # 5. ê²°ê³¼ ë°˜í™˜
        return {
            'evaluation': overall_evaluation,
            'approved_questions': [eq['question'] for eq in approved],
            'rejected_questions': [eq['question'] for eq in rejected],
            'suggestions': suggestions,
            'overall_score': overall_evaluation['average_score'],
            'metadata': {
                'total_evaluated': len(questions),
                'approved_count': len(approved),
                'rejected_count': len(rejected),
                'evaluation_time': datetime.now().isoformat()
            }
        }
    
    def _evaluate_question(self, question: Dict, difficulty: str, 
                          interest: str, age_group: str) -> Dict:
        """ê°œë³„ ë¬¸ì œ í‰ê°€"""
        
        scores = {}
        feedback = []
        
        # 1. ë‚œì´ë„ ì ì ˆì„± í‰ê°€
        difficulty_score = self._check_difficulty_match(question, difficulty)
        scores['difficulty_match'] = difficulty_score
        if difficulty_score < 0.7:
            feedback.append(f"ë‚œì´ë„ê°€ {difficulty} ìˆ˜ì¤€ì— ë§ì§€ ì•ŠìŒ")
        
        # 2. ê´€ì‹¬ì‚¬ ê´€ë ¨ì„± í‰ê°€
        relevance_score = self._check_content_relevance(question, interest)
        scores['content_relevance'] = relevance_score
        if relevance_score < 0.7:
            feedback.append(f"{interest} ì£¼ì œì™€ ê´€ë ¨ì„± ë¶€ì¡±")
        
        # 3. ì–¸ì–´ì  í’ˆì§ˆ í‰ê°€
        linguistic_score = self._check_linguistic_quality(question)
        scores['linguistic_quality'] = linguistic_score
        if linguistic_score < 0.7:
            feedback.append("ë¬¸ë²•ì´ë‚˜ ì² ì ì˜¤ë¥˜ ê°€ëŠ¥ì„±")
        
        # 4. êµìœ¡ì  ê°€ì¹˜ í‰ê°€
        educational_score = self._check_educational_value(question, difficulty)
        scores['educational_value'] = educational_score
        if educational_score < 0.7:
            feedback.append("êµìœ¡ì  ê°€ì¹˜ ë¶€ì¡±")
        
        # 5. í˜•ì‹ ì •í™•ì„± í‰ê°€
        format_score = self._check_format_correctness(question)
        scores['format_correctness'] = format_score
        if format_score < 0.7:
            feedback.append("ë¬¸ì œ í˜•ì‹ ì˜¤ë¥˜")
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_score = sum(
            scores[criterion] * self.evaluation_criteria[criterion]['weight']
            for criterion in scores
        )
        
        return {
            'scores': scores,
            'total_score': total_score,
            'feedback': feedback,
            'passed': total_score >= 0.6
        }
    
    def _check_difficulty_match(self, question: Dict, difficulty: str) -> float:
        """ë‚œì´ë„ ì ì ˆì„± í™•ì¸"""
        
        standards = self.difficulty_standards[difficulty]
        score = 1.0
        
        # ë¬¸ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = ""
        if question['type'] == 'multiple_choice':
            text = question.get('question', '')
        elif question['type'] == 'fill_blank':
            text = question.get('sentence', '')
        elif question['type'] == 'true_false':
            text = question.get('statement', '')
        elif question['type'] == 'translation':
            text = question.get('source', '')
        elif question['type'] == 'reading_comprehension':
            text = question.get('passage', '')
        
        # ë¬¸ì¥ ê¸¸ì´ ì²´í¬
        if len(text) > standards['max_sentence_length'] * 1.5:
            score -= 0.3
        elif len(text) < standards['max_sentence_length'] * 0.3:
            score -= 0.2
        
        # ë³µì¡ë„ ì²´í¬ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        if difficulty == 'beginner':
            # ì´ˆê¸‰ì€ ë‹¨ìˆœí•´ì•¼ í•¨
            if text.count(',') > 2 or text.count('.') > 2:
                score -= 0.2
        elif difficulty == 'intermediate':
            # ì¤‘ê¸‰ì€ ì ë‹¹í•œ ë³µì¡ë„
            if text.count(',') < 1 and text.count('.') < 1:
                score -= 0.2
        elif difficulty == 'advanced':
            # ê³ ê¸‰ì€ ë³µì¡í•´ì•¼ í•¨
            if text.count(',') < 2 and len(text) < 50:
                score -= 0.3
        
        return max(0, min(1, score))
    
    def _check_content_relevance(self, question: Dict, interest: str) -> float:
        """ê´€ì‹¬ì‚¬ ê´€ë ¨ì„± í™•ì¸"""
        
        # ë¬¸ì œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        all_text = []
        for key in ['question', 'sentence', 'statement', 'source', 'passage']:
            if key in question:
                all_text.append(str(question[key]))
        
        combined_text = ' '.join(all_text).lower()
        
        # ê´€ì‹¬ì‚¬ í‚¤ì›Œë“œ ì²´í¬
        interest_keywords = {
            'kpop': ['ì•„ì´ëŒ', 'ê°€ìˆ˜', 'ìŒì•…', 'ë…¸ë˜', 'ëŒ„ìŠ¤', 'k-pop', 'kpop', 'ê·¸ë£¹'],
            'kdrama': ['ë“œë¼ë§ˆ', 'ë°°ìš°', 'ì—°ê¸°', 'ì‹œì²­ë¥ ', 'ë°©ì†¡'],
            'korean_food': ['ìŒì‹', 'ìš”ë¦¬', 'ë§›', 'ê¹€ì¹˜', 'ë°¥', 'ë°˜ì°¬'],
            'korean_culture': ['ë¬¸í™”', 'ì „í†µ', 'í•œë³µ', 'ëª…ì ˆ', 'ì˜ˆì ˆ'],
            'technology': ['ê¸°ìˆ ', 'ìŠ¤ë§ˆíŠ¸í°', 'ì»´í“¨í„°', 'ì¸í„°ë„·', 'IT'],
            'sports': ['ìŠ¤í¬ì¸ ', 'ìš´ë™', 'ê²½ê¸°', 'ì„ ìˆ˜', 'íŒ€']
        }
        
        keywords = interest_keywords.get(interest, [interest])
        matches = sum(1 for keyword in keywords if keyword in combined_text)
        
        # ë§¤ì¹­ ë¹„ìœ¨ë¡œ ì ìˆ˜ ê³„ì‚°
        score = min(1.0, matches / max(1, len(keywords) * 0.3))
        
        return score
    
    def _check_linguistic_quality(self, question: Dict) -> float:
        """ì–¸ì–´ì  í’ˆì§ˆ í™•ì¸"""
        
        # LLMì„ ì‚¬ìš©í•œ í’ˆì§ˆ í‰ê°€
        text_samples = []
        for key in ['question', 'sentence', 'statement', 'source']:
            if key in question:
                text_samples.append(question[key])
        
        if not text_samples:
            return 0.5
        
        sample = text_samples[0][:100]  # ì²« 100ìë§Œ í‰ê°€
        
        prompt = f"""
ë‹¤ìŒ í•œêµ­ì–´ ë¬¸ì¥ì˜ ë¬¸ë²•ê³¼ ì² ìë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.
ë¬¸ì¥: {sample}

í‰ê°€ (0-10ì ):
- ë¬¸ë²• ì •í™•ì„±:
- ì² ì ì •í™•ì„±:
- ìì—°ìŠ¤ëŸ¬ì›€:

ì¢…í•© ì ìˆ˜ (0-10):
"""
        
        response = self.generate_response(prompt, max_new_tokens=50)
        
        # ì ìˆ˜ ì¶”ì¶œ ì‹œë„
        try:
            if 'ì¢…í•©' in response:
                score_text = response.split('ì¢…í•©')[1]
                for word in score_text.split():
                    if word.replace('.', '').isdigit():
                        return float(word) / 10
        except:
            pass
        
        # ê¸°ë³¸ ì ìˆ˜
        return 0.7
    
    def _check_educational_value(self, question: Dict, difficulty: str) -> float:
        """êµìœ¡ì  ê°€ì¹˜ í‰ê°€"""
        
        score = 0.7  # ê¸°ë³¸ ì ìˆ˜
        
        # ë¬¸ì œ ìœ í˜•ë³„ ê°€ì¹˜ í‰ê°€
        q_type = question.get('type')
        
        if q_type == 'multiple_choice':
            # ì„ íƒì§€ê°€ êµìœ¡ì ì¸ì§€ í™•ì¸
            options = question.get('options', [])
            if len(options) >= 4:
                score += 0.1
            if question.get('explanation'):
                score += 0.2
        
        elif q_type == 'fill_blank':
            # íŒíŠ¸ê°€ ìˆìœ¼ë©´ ê°€ì¹˜ ìƒìŠ¹
            if question.get('hints'):
                score += 0.2
        
        elif q_type == 'translation':
            # ëŒ€ì²´ ë²ˆì—­ì´ ìˆìœ¼ë©´ ê°€ì¹˜ ìƒìŠ¹
            if question.get('alternatives'):
                score += 0.15
        
        elif q_type == 'reading_comprehension':
            # ë…í•´ ë¬¸ì œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë†’ì€ ê°€ì¹˜
            score = 0.9
            if len(question.get('questions', [])) >= 2:
                score = 1.0
        
        # ë‚œì´ë„ì— ë§ëŠ” í¬ì¸íŠ¸ ë°°ì  í™•ì¸
        expected_points = {
            'beginner': {'min': 3, 'max': 7},
            'intermediate': {'min': 5, 'max': 10},
            'advanced': {'min': 7, 'max': 15}
        }
        
        points = question.get('points', 0)
        if expected_points[difficulty]['min'] <= points <= expected_points[difficulty]['max']:
            score += 0.1
        
        return min(1.0, score)
    
    def _check_format_correctness(self, question: Dict) -> float:
        """í˜•ì‹ ì •í™•ì„± í™•ì¸"""
        
        score = 1.0
        q_type = question.get('type')
        
        # í•„ìˆ˜ í•„ë“œ ì²´í¬
        required_fields = {
            'multiple_choice': ['question', 'options', 'answer'],
            'fill_blank': ['sentence', 'answer'],
            'true_false': ['statement', 'answer'],
            'translation': ['source', 'answer'],
            'reading_comprehension': ['passage', 'questions']
        }
        
        if q_type in required_fields:
            for field in required_fields[q_type]:
                if field not in question:
                    score -= 0.3
        else:
            score = 0.5  # ì•Œ ìˆ˜ ì—†ëŠ” ìœ í˜•
        
        # íŠ¹ì • ìœ í˜•ë³„ ì¶”ê°€ ì²´í¬
        if q_type == 'multiple_choice':
            options = question.get('options', [])
            if len(options) < 2:
                score -= 0.5
            elif len(options) > 6:
                score -= 0.2
        
        elif q_type == 'fill_blank':
            sentence = question.get('sentence', '')
            if '_____' not in sentence and '___' not in sentence:
                score -= 0.5
        
        return max(0, score)
    
    def _evaluate_overall(self, evaluated_questions: List[Dict], 
                         difficulty: str, interest: str) -> Dict:
        """ì „ì²´ ë¬¸ì œ ì„¸íŠ¸ í‰ê°€"""
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        total_scores = [eq['score'] for eq in evaluated_questions]
        average_score = sum(total_scores) / len(total_scores) if total_scores else 0
        
        # ë¬¸ì œ ìœ í˜• ë¶„í¬ ë¶„ì„
        type_distribution = {}
        for eq in evaluated_questions:
            q_type = eq['question'].get('type', 'unknown')
            type_distribution[q_type] = type_distribution.get(q_type, 0) + 1
        
        # ë‚œì´ë„ ì¼ê´€ì„± ë¶„ì„
        difficulty_scores = [
            eq['evaluation']['scores'].get('difficulty_match', 0) 
            for eq in evaluated_questions
        ]
        difficulty_consistency = sum(difficulty_scores) / len(difficulty_scores) if difficulty_scores else 0
        
        # ê´€ì‹¬ì‚¬ ê´€ë ¨ì„± ë¶„ì„
        relevance_scores = [
            eq['evaluation']['scores'].get('content_relevance', 0)
            for eq in evaluated_questions
        ]
        content_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0
        
        return {
            'average_score': average_score,
            'type_distribution': type_distribution,
            'difficulty_consistency': difficulty_consistency,
            'content_relevance': content_relevance,
            'total_questions': len(evaluated_questions),
            'passed_questions': sum(1 for eq in evaluated_questions if eq['score'] >= 0.6),
            'failed_questions': sum(1 for eq in evaluated_questions if eq['score'] < 0.6)
        }
    
    def _generate_suggestions(self, evaluated_questions: List[Dict], 
                            difficulty: str, interest: str, 
                            overall_evaluation: Dict) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        
        suggestions = []
        
        # 1. ì „ì²´ ì ìˆ˜ ê¸°ë°˜ ì œì•ˆ
        if overall_evaluation['average_score'] < 0.6:
            suggestions.append("âš ï¸ ì „ë°˜ì ì¸ ë¬¸ì œ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        elif overall_evaluation['average_score'] < 0.8:
            suggestions.append("ğŸ“ ë¬¸ì œ í’ˆì§ˆì´ ì–‘í˜¸í•˜ë‚˜ ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            suggestions.append("âœ… ë¬¸ì œ í’ˆì§ˆì´ ìš°ìˆ˜í•©ë‹ˆë‹¤.")
        
        # 2. ë‚œì´ë„ ì¼ê´€ì„± ì œì•ˆ
        if overall_evaluation['difficulty_consistency'] < 0.7:
            suggestions.append(f"ğŸ¯ {difficulty} ë‚œì´ë„ì— ë§ê²Œ ë¬¸ì œë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
            if difficulty == 'beginner':
                suggestions.append("   - ë¬¸ì¥ì„ ë” ê°„ë‹¨í•˜ê²Œ ë§Œë“œì„¸ìš”")
                suggestions.append("   - ê¸°ì´ˆ ì–´íœ˜ë§Œ ì‚¬ìš©í•˜ì„¸ìš”")
            elif difficulty == 'intermediate':
                suggestions.append("   - ì ì ˆí•œ ë³µì¡ë„ë¥¼ ìœ ì§€í•˜ì„¸ìš”")
                suggestions.append("   - ë‹¤ì–‘í•œ ë¬¸ë²• êµ¬ì¡°ë¥¼ í¬í•¨í•˜ì„¸ìš”")
            else:
                suggestions.append("   - ê³ ê¸‰ ì–´íœ˜ì™€ ê´€ìš©êµ¬ë¥¼ ì¶”ê°€í•˜ì„¸ìš”")
                suggestions.append("   - ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
        
        # 3. ê´€ì‹¬ì‚¬ ê´€ë ¨ì„± ì œì•ˆ
        if overall_evaluation['content_relevance'] < 0.7:
            suggestions.append(f"ğŸ­ {interest} ì£¼ì œì™€ ë” ê´€ë ¨ëœ ë‚´ìš©ì„ í¬í•¨í•˜ì„¸ìš”.")
            suggestions.append(f"   - {interest} ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ë” ë§ì´ ì‚¬ìš©í•˜ì„¸ìš”")
            suggestions.append(f"   - ì‹¤ì œ {interest} ì˜ˆì‹œë¥¼ í™œìš©í•˜ì„¸ìš”")
        
        # 4. ë¬¸ì œ ìœ í˜• ë¶„í¬ ì œì•ˆ
        type_dist = overall_evaluation['type_distribution']
        total = overall_evaluation['total_questions']
        
        # ì˜ˆìƒ ë¶„í¬ì™€ ë¹„êµ
        expected_dist = self.difficulty_configs[difficulty]
        
        for q_type in ['multiple_choice', 'fill_blank', 'true_false', 'translation', 'reading_comprehension']:
            actual_ratio = type_dist.get(q_type, 0) / total * 100 if total > 0 else 0
            expected_ratio = expected_dist.get(q_type, 0)
            
            if abs(actual_ratio - expected_ratio) > 15:  # 15% ì´ìƒ ì°¨ì´
                if actual_ratio < expected_ratio:
                    suggestions.append(f"ğŸ“Š {q_type} ë¬¸ì œë¥¼ ë” ì¶”ê°€í•˜ì„¸ìš” (í˜„ì¬: {actual_ratio:.0f}%, ê¶Œì¥: {expected_ratio}%)")
                else:
                    suggestions.append(f"ğŸ“Š {q_type} ë¬¸ì œë¥¼ ì¤„ì´ì„¸ìš” (í˜„ì¬: {actual_ratio:.0f}%, ê¶Œì¥: {expected_ratio}%)")
        
        # 5. ê°œë³„ ë¬¸ì œ í”¼ë“œë°± ì¢…í•©
        common_issues = {}
        for eq in evaluated_questions:
            for feedback in eq['evaluation']['feedback']:
                common_issues[feedback] = common_issues.get(feedback, 0) + 1
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ë¬¸ì œ top 3
        if common_issues:
            sorted_issues = sorted(common_issues.items(), key=lambda x: x[1], reverse=True)
            suggestions.append("\nğŸ” ì£¼ìš” ê°œì„  ì‚¬í•­:")
            for issue, count in sorted_issues[:3]:
                suggestions.append(f"   - {issue} ({count}ê°œ ë¬¸ì œ)")
        
        # 6. ì‹¤íŒ¨í•œ ë¬¸ì œë“¤ì— ëŒ€í•œ êµ¬ì²´ì  ì œì•ˆ
        failed_questions = [eq for eq in evaluated_questions if eq['score'] < 0.6]
        if failed_questions:
            suggestions.append(f"\nâŒ {len(failed_questions)}ê°œ ë¬¸ì œê°€ ê¸°ì¤€ ë¯¸ë‹¬:")
            for i, fq in enumerate(failed_questions[:3], 1):  # ìƒìœ„ 3ê°œë§Œ
                q_type = fq['question'].get('type', 'unknown')
                score = fq['score']
                suggestions.append(f"   {i}. {q_type} ë¬¸ì œ (ì ìˆ˜: {score:.2f})")
                if fq['evaluation']['feedback']:
                    suggestions.append(f"      - {fq['evaluation']['feedback'][0]}")
        
        return suggestions