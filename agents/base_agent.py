"""
base_agent.py - ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤
"""
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BaseAgent(ABC):
    """ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = None, agent_name: str = "BaseAgent"):
        """
        BaseAgent ì´ˆê¸°í™”
        
        Args:
            model_name: Hugging Face ëª¨ë¸ ì´ë¦„
            agent_name: ì—ì´ì „íŠ¸ ì‹ë³„ ì´ë¦„
        """
        self.agent_name = agent_name
        
        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
        if model_name is None:
            model_name = "skt/kogpt2-base-v2"  # ê°€ë²¼ìš´ í•œêµ­ì–´ ëª¨ë¸
        
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"ğŸš€ {self.agent_name} ì´ˆê¸°í™”: {model_name}")
        logger.info(f"ğŸ’» ë””ë°”ì´ìŠ¤: {self.device}")
        
        self._load_model()
    
    def _load_model(self):
        """Hugging Faceì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            logger.info(f"ğŸ“¥ {self.agent_name} ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            # íŒŒì´í”„ë¼ì¸ ìƒì„±
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if self.device == "cuda" else -1
            )
            
            logger.info(f"âœ… {self.agent_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ {self.agent_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def generate_response(self, prompt: str, max_new_tokens: int = 256, temperature: float = 0.7) -> str:
        """
        í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„ (ì°½ì˜ì„± ì¡°ì ˆ)
        
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        try:
            formatted_prompt = self._format_prompt(prompt)
            
            if self.pipeline:
                outputs = self.pipeline(
                    formatted_prompt,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=0.9,
                    do_sample=True,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    return_full_text=False
                )
                response = outputs[0]['generated_text']
            else:
                inputs = self.tokenizer(
                    formatted_prompt,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=1024
                ).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=0.9,
                        do_sample=True,
                        repetition_penalty=1.1
                    )
                
                generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            return self._clean_output(response)
            
        except Exception as e:
            logger.error(f"âŒ {self.agent_name} ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return ""
    
    def _format_prompt(self, prompt: str) -> str:
        """ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
        model_lower = self.model_name.lower()
        
        if 'kogpt' in model_lower:
            return f"ì§ˆë¬¸: {prompt}\në‹µë³€:"
        elif 'polyglot' in model_lower:
            return f"### ì§ˆë¬¸:\n{prompt}\n\n### ì‘ë‹µ:\n"
        else:
            return prompt
    
    def _clean_output(self, output: str) -> str:
        """ì¶œë ¥ ì •ë¦¬"""
        if not output:
            return ""
        
        # íŠ¹ìˆ˜ í† í° ì œê±°
        special_tokens = ["<|endoftext|>", "</s>", "###", "\n\n\n"]
        for token in special_tokens:
            output = output.replace(token, "")
        
        return output.strip()
    
    @abstractmethod
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê° ì—ì´ì „íŠ¸ë³„ ì²˜ë¦¬ ë¡œì§ (ìƒì†ë°›ì•„ êµ¬í˜„)
        
        Args:
            input_data: ì…ë ¥ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        pass
    
    def validate_input(self, input_data: Dict[str, Any], required_fields: list) -> bool:
        """
        ì…ë ¥ ë°ì´í„° ê²€ì¦
        
        Args:
            input_data: ì…ë ¥ ë°ì´í„°
            required_fields: í•„ìˆ˜ í•„ë“œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ê²€ì¦ ì„±ê³µ ì—¬ë¶€
        """
        for field in required_fields:
            if field not in input_data:
                logger.error(f"âŒ {self.agent_name}: í•„ìˆ˜ í•„ë“œ '{field}' ëˆ„ë½")
                return False
        return True